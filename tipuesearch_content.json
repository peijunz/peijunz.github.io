{"pages":[{"title":"Spanning Tree Protocol Testing Code","text":"Download Here Copy your Switch.py to this test suite folder only . Do not overwrite Topology.py Simply run python check_answer.py to test randomly generated cases","tags":"pages","url":"https://www.peijun.me/pages/stptest.html"},{"title":"量子态与测量","text":"写给了解一些量子力学基本概念的人，理清一些概念。 $\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\newcommand{\\inv}[1]{#1&#94;{-1}} % Inverse Matrix \\newcommand{\\invt}[1]{#1&#94;{-T}} % Inverse Transposed Matrix \\renewcommand{\\nl}{\\\\&\\phantom{{}={}}}% Newline In aligned equations \\newcommand{\\pfr}[2]{\\frac{\\pp #1}{\\pp #2}} % Partial derivative \\newcommand{\\dfr}[2]{\\frac{\\dd #1}{\\dd #2}} % Total derivative \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS} \\newcommand{\\nl}{\\\\&\\phantom{={}}} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta}$ # 单粒子与测量 考虑一个量子态所处的希尔伯特空间，它存在完备正交的基矢集合$\\{\\ket{i}\\}, i=0,1,2\\ldots$ 对任何一个量子态$\\ket{\\psi}$，我们可以做分解$\\ket{\\psi}=\\sum c_i\\ket{i}, c_i\\in \\mathbb{C}$ 我们可以称$\\ket{\\psi}$是基矢$\\{\\ket{i}\\}$的叠加态 单粒子测量 可观测量的算符，可以在基矢$\\{\\ket{i}\\}$下把它写成一个矩阵$\\Omega$ 矩阵$\\Omega$，可以得到特征值$\\omega_i$以及特征向量$\\ket{\\omega_i}$ 测量到，可观测量处于$\\omega_i$值的概率是$|\\braket{\\psi|\\omega_i}|&#94;2$ 可观测量的期望值是$\\braket{\\psi|\\Omega|\\psi}=\\sum |\\braket{\\psi|\\omega_i}|&#94;2 \\omega_i$ # 二体系统 我们考虑两个量子系统$L$和$R$，他们的希尔伯特空间大小分别是$m$和$n$ 可以把$L$当作感兴趣的系统，而把$R$当作以外的环境，就可以推广到 任意多粒子 。例如，给定五个粒子，可以规定第1，2个属于$L$，剩下的都属于$R$ $L$系统有基矢$\\{{\\ket{0}_L, \\ket{1}_L,\\ldots\\ket{m-1}_L}\\}$ $R$系统有基矢$\\{{\\ket{0}_R, \\ket{1}_R,\\ldots\\ket{n-1}_R}\\}$ 假设$L$系统处于$\\ket{i}$状态，并且$R$系统处于$\\ket{j}$状态，那么二体量子态是简单的$\\ket{i}\\otimes\\ket{j}$ 二体系统的正交完备 基矢 ，可以选取为$\\{{\\ket{i}\\otimes\\ket{j}}\\}， 0\\leq i<m, 0\\leq j<n$ 二体系统的 任意态 $\\ket{\\Psi}$都可以用这组基展开为$$\\ket{\\Psi}=\\sum_{ij} m_{ij}\\ket{i}_L\\otimes\\ket{j}_R$$ $\\ket{\\Psi}$可以映射到一个$m\\times n$的矩阵$$M=\\begin{bmatrix}m_{00} & m_{01}&\\ldots\\\\ m_{10}&m_{11}&\\ldots\\\\\\vdots&\\vdots&\\ddots\\end{bmatrix}$$ 积态 $\\ket{\\psi}_L\\otimes\\ket{\\phi}_R$，指左系统处于$\\ket{\\psi}=\\sum l_i\\ket{i}$态，而且右系统处于$\\ket{\\phi}=\\sum r_i\\ket{j}$态 积态$\\ket{\\psi}_L\\otimes\\ket{\\phi}_R$对应的矩阵是$$M=\\begin{bmatrix}l_0\\\\l_1\\\\\\vdots\\end{bmatrix}\\begin{bmatrix}r_0 & r_1 & \\ldots\\end{bmatrix}$$ 对积态的矩阵做SVD(奇异值分解)，那么它只有一个模为1的奇异值 纠缠态 ：不可以分解为积态的叫做纠缠态 # 二体系统的测量 测量 $L$系统的可观测量$\\Omega$，在两体系统中变成了$\\Omega\\otimes I$。它的期望值是$\\langle\\Omega\\rangle=\\braket{\\Psi|\\Omega\\otimes I|\\Psi}=\\sum m_{ij}m_{jk}&#94;*\\braket{k|\\Omega|i}$ 密度矩阵 定义左系统的密度矩阵$\\rho_L=\\sum m_{ij}m_{jk}&#94;*\\ket{i}\\bra{k}$，那么上述$\\langle\\Omega\\rangle=\\tr [\\rho_L \\Omega]$ 假设系数矩阵的奇异值分解是$M=U\\Lambda V&#94;\\dagger$，那么$\\rho_L=MM&#94;\\dagger=U\\Lambda&#94;2U&#94;\\dagger$正是$\\rho_L$的特征值分解。 设$\\Lambda=\\mathrm{diag}[\\lambda_0, \\lambda_1,\\ldots \\lambda_k]$，则$\\rho_L$的非零特征值$p_i=\\lambda_i&#94;2$ 纯态与混态 密度矩阵特征分解后，可以得到$\\rho=\\sum p_i\\ket{\\phi_i}\\bra{\\phi_i}$，其中$\\ket{\\phi_i}$对应于$U$矩阵的第$i$列向量 纯态 ：如果某系统的密度矩阵非零特征值的数量为1，即$\\rho=\\ket{\\psi}\\bra{\\psi}$，则该系统处于纯态$\\ket{\\psi}$，与环境不存在纠缠。 混态 ：非纯态则为混态 纠缠熵 $S=-\\tr[\\rho_L\\ln \\rho_L]=-\\tr[\\rho_R\\ln \\rho_R] =-\\sum p_i\\ln p_i$","tags":"Physics","url":"https://www.peijun.me/quantum-state-and-measurement.html"},{"title":"Master Theorem on Recursion","text":"Given $$T[n]=\\sum a_i T[n/b_i]+f(n),\\quad a_i>0, b_i>1,$$ we want to find asymptotic expression for $T[n]$ # Critical exponent for $f=0$ When $f(n)=0$, we make the hypothesis that $T[n]=an&#94;c_0$. From recursion formula, we find $\\sum a_i/b_i&#94;{c_0}=1$ # Case $f(n)\\leq kn&#94;{c}, c<c_0$ Define $$A=k\\big/\\Big[\\sum a_i/b_i&#94;c-1\\Big]>0,\\quad S[n]=T[n]+An&#94;c$$ We have $$S[n]=\\sum a_i S[n/b_i]$$ From our $f=0$ case analysis, $S[n]=\\Theta(n&#94;{c_0}), T[n]=\\Theta(n&#94;{c_0})-An&#94;c=\\Theta(n&#94;{c_0})$ # Case $f(n)\\geq kn&#94;c, c>c_0$ Define $$B=-A=k\\big/\\Big[1-\\sum a_i/b_i&#94;c\\Big]>0,\\quad S[n]=T[n]-Bn&#94;c$$ We have $$S[n]=\\sum a_i S[n/b_i]$$ From our $f=0$ case analysis, $S[n]=\\Theta(n&#94;{c_0})$, $T[n]=\\Theta(n&#94;{c_0})+Bn&#94;c=\\Theta(n&#94;c)$ # Case $f(n)=kn&#94;{c_0}\\log&#94;k n, k\\geq 0 $ Use ansatz $T[n]=kn&#94;{c_0}\\log&#94;{k+1} n$, we find $T[n]-\\sum a_i T[n/b_i]=\\Theta(n&#94;{c_0}\\log&#94;k n)$. So the ansatz is the correct solution.","tags":"Computer","url":"https://www.peijun.me/master-theorem-on-recursion.html"},{"title":"Expectation value of quickselect runtime","text":"# Problem Suppose there are $n$ numbers, use Quickselect to determine $k$-th smallest number. What is average runtime? # Recursion For our convenience, we assume there are no equal numbers. Initially, there are $l=k-1$ numbers smaller than target and $r=n-k$ numbers larger than target, which satisfy $n=l+r+1$. We denote expectation value of run time for this situation by $T[l, r]$. For one step of quickselect , it takes $cn$ time to do partition with a random pivot. Then the problem falls into three cases: If pivot is smaller than target, $l$ is reduced and falls in the range $[0, l-1]$ If pivot is greater than target, $r$ is reduced and falls in the range $[0, r-1]$ If pivot is target, our algorithm halts, which takes constant time $d$. With this analysis, we find recursion for $T[l, r]$: $$T[l, r]= cn+\\frac{\\sum_{k=0}&#94;{l-1} T[k, r]+\\sum_{k=0}&#94;{r-1} T[l, k]+d}{n}$$ # Local Difference The recursion of $T$ is equivalent to $$nT[l, r]= cn&#94;2+d+\\sum_{k=0}&#94;{l-1} T[k, r]+\\sum_{k=0}&#94;{r-1} T[l, k]$$ Similarly we have \\begin{align} (n-1)T[l, r-1]&= c(n-1)&#94;2+d+\\sum_{k=0}&#94;{l-1} T[k, r-1]+\\sum_{k=0}&#94;{r-2} T[l, k]\\\\ (n-1)T[l-1, r]&= c(n-1)&#94;2+d+\\sum_{k=0}&#94;{l-2} T[k, r]+\\sum_{k=0}&#94;{r-1} T[l-1, k]\\\\ (n-2)T[l-1, r-1]&= c(n-2)&#94;2+d+\\sum_{k=0}&#94;{l-2} T[k, r-1]+\\sum_{k=0}&#94;{r-2} T[l-1, k] \\end{align} Evaluate $nT[l, r]-(n-1)T[l, r-1]-(n-1)T[l-1, r]+(n-2)T[l-1, r-1]$, and we find local difference term for $(l, r)$: $$T[l, r]-T[l, r-1]-T[l-1, r]+T[l-1, r-1]=2c/n$$ # Summation Sum over all local differences for $(i, j)$ points ($0< i\\leq l, 0<j\\leq r$), and we have \\begin{align} &\\phantom{{}={}}T[l, r]-T[l, 0]-T[0, r]+T[0, 0]\\\\ &=\\sum_{i=1}&#94;l\\sum_{j=1}&#94;r\\frac{2c}{i+j+1}\\\\ &< 2c\\int_{1/2}&#94;{l+1/2}\\int_{1/2}&#94;{r+1/2} \\frac{dxdy}{x+y+1}\\\\ &= 2c[(l+r+2)\\ln (l+r+2)-(l+2)\\ln (l+2)-(r+2)\\ln (r+2)+2\\ln 2]\\\\ &= 2c\\Big[N[-p\\ln (p+1/N)-q\\ln (q+1/N)]-\\ln (l+2)(r+2)+2\\ln 2\\Big]\\\\ &< 2cSN,\\quad S=-p\\ln p-q\\ln q \\end{align} where $N=l+r+2, p=(l+1)/N, q=(r+1)/N$. $S$ is entropy for $(p, q)$ distribution. # Complexity It is obvious that $T[n, 0]=T[0, n]=bn, T[0, 0]=d$, which gives $$T[l, r]= (b+2cS)N$$ It shows that constant factor of computing median is larger than factor for max/min. # $T/N$ as a function of percentage $p$ Assume $b=c=1$, then we can plot $T/N$ as function of $p$ In [25]: p = linspace ( 1e-6 , 1 - 1e-6 , 101 ) q = 1 - p b = c = 1 plot ( p , b - 2 * c * ( p * log ( p ) + q * log ( q ))) grid (); xlabel ( 'Percentage' ) ylabel ( r '$T/N$' );","tags":"Computer","url":"https://www.peijun.me/quickselect-complexity.html"},{"title":"迁移到Manjaro系统了","text":"# 起因 由于openSUSE软件更新实在太慢以及对双显卡支持太差，暑假研究了下其他的滚动Linux发行版。以下是对比： openSUSE的优点是稳定，缺点简直不可胜数： 软件更新太慢（如火狐，goldendict等） 需要加的源太多太麻烦，受版权掣肘，连多媒体源都不自带，更别提其他的了 系统额外东西过多，不够KISS Arch Linux安装太复杂，但是有巨大的AUR软件源，稳定性不好容易滚挂 Manjaro目前看来几乎没有缺点，而优点简直太多了 安装比较简单 系统稳定性足够好，内核更新的并不是很快，可以自由锁定/切换内核 驱动支持非常好 常用软件更新够快 软件足够丰富，继承Archlinux的AUR KDE用起来不错 易用性好，可配置性很强 # 安装 也许因为Manjaro的默认桌面是XFCE版本，它的KDE版本在我的电脑上总是安装失败。最终，我安装好了xfce版本。总的来说安装过程比openSUSE折腾一些，但是有惊无险。 # 启动 不知道什么原因，Manjaro的UEFI条目在安装好后并未被添加。于是我手动运行 efibootmgr 命令成功添加Manjaro至efi的F12启动菜单。 # Install Manjaro item efibootmgr -c -d /dev/nvme0n1 -l '\\EFI\\Manjaro\\bootx64.efi' -L Manjaro # Confirm Manjaro is added efibootmgr -v # Delete openSUSE item # efibootmgr -b <boot_num_for_opensuse> -B # 桌面 安装KDE Plasma 5 后面的sddm和octopi部分照做。 重启成功后，最终移除xfce sudo pacman -Rc xfce4 xfce4-goodies sudo pacman -Rc lightdm pamac # 输入法 安装命令： sudo pacman -S fcitx-im kcm-fcitx 安装好了fcitx后，需要在 /etc/profile 中加入 GTK_IM_MODULE = fcitx QT_IM_MODULE = fcitx XMODIFIERS = @im = fcitx # 软件 Manjaro装好后，所带的软件源本身内容已经很丰富了。除此之外，安装 yaourt 后可以使用巨大的AUR源，但缺点是每次得重新编译。比AUR更方便的是， archlinuxcn源 预编译好了一些aur中常用的包，可以直接用pacman安装。 # 硬件 # N卡 请参考： Bumblebee and Steam # 安装CUDA pacman -S cuda # 触屏 参考 Wacom Tablet And Pen pacman -S extra/xf86-input-wacom pacman -S community/kcm-wacomtablet 查看并禁用手势 xsetwacom --list devices xsetwacom --set \"Wacom HID 50F8 Finger touch\" Gesture off # 小技巧 /etc/skel/ 中包含了默认的manjaro配置文件 cp -r /etc/skel/. ~/","tags":"Computer","url":"https://www.peijun.me/migration-to-manjaro.html"},{"title":"链表的伪头节点","text":"本文介绍C语言中，如何无中生有，给链表创造出一个不占空间的伪头节点 # 基本定义 设我们有一个不带空头节点的链表，其节点定义为 //typedef int dtype; typedef struct node { dtype data ; struct node * next ; } node ; 链表定义为 typedef struct linklist { node * head ; //... } linklist ; # 空头节点 # 常驻的空头节点 设现在有一个链表 linklist *L ，那么它的头节点的指针为 L->head 。如果我们要写一个对链表进行修改的功能，那么 L->head 本身可能也会被修改。 如果给链表本身，引入空头节点，可以使得 每个有效元素都有前置元素 ，方便更新链表。但是此种方法有两个缺点： 头节点需要一个空的 node.data 域，当此项体积比较庞大的时候，是对空间的浪费 如果只需要对链表进行读取而不需要修改，那么需要额外的代码提取真正的头节点 # 临时空头节点 解决第二个问题的可行方案是，让空头节点不常驻链表。每次仅当需要修改链表的时候，临时产生一个空头节点。此方案同时部分解决了第一个问题，但是临时修改时，依然存在不必要的空间浪费。 由于我们并不对空头节点 pred 的 pred->next 以外的域进行更改，本质上 pred->data 域是纯粹的空间浪费。 # 临时伪空头节点 我们注意到： 只需要存在 next 域的存储空间，就可以根据 next 域的地址，推算出一个不存在的假的结构体地址。 以下 struct_addr_from_elem 宏，便可以根据元素的地址推算出结构体的地址 #define struct_addr_from_elem(struct_, elem, obj) (void*)&(obj) - offsetof((struct_), (elem)) 那么我们可以由此轻松产生空头节点指针 pred ： linklist * L ; node * prev = struct_addr_from_elem ( node , next , L -> head ); # 实例 对不带头节点的链表插入元素，并使得链表元素保持升序。 node * init_node ( dtype data , node * next ); void ascend_insert ( linklist * L , dtype data ){ //初始化空头节点指针和头指针 node * prev = struct_addr_from_elem ( node , next , L -> head ), * p = L -> head ; //找到插入点 while ( p && ( data > p -> data )){ prev = p ; p = p -> next ; } //插入 prev -> next = init_node ( data , p ); }","tags":"Computer","url":"https://www.peijun.me/fake-header-node.html"},{"title":"整数幂的求和","text":"# 问题 计算对于给定的整数$k$，求$f(n, k)=\\sum_{i=1}&#94;n i&#94;k$关于$n$的多项式表达式。例如 \\begin{align} f(n, 0)&=n\\\\ f(n, 1)&=n(n+1)/2\\\\ f(n, 2)&=n(n+1)(2n+1)/6\\\\ f(n, 3)&=n&#94;2(n+1)&#94;2/4 \\end{align} # 差分的等价描述 基于差分，我们可以将$f(n, k)$转换为等价形式，初值条件和递推关系 \\begin{align} f(1, k)&=1\\\\ \\Delta f(n, k) &= f(n, k)-f(n-1, k)=n&#94;k \\end{align} 我们也可以通过递推关系定义初值$f(0, k)=0$来代替$f(1, k)=1$ # 拟设(Ansatz) 我们拟设$f(n, k)$有形如$\\sum_{i=0}&#94;{k+1} c_i&#94;k n&#94;i$的待定系数解。 代入初值条件$f(0, k)$，我们就可以得到 $$c_0&#94;k=0$$ 代入递推关系，可得 \\begin{align} n&#94;k&=\\Delta f(n, k)\\\\ &=\\sum_{i=0}&#94;{k+1} c_i&#94;k [n&#94;i-(n-1)&#94;i]\\\\ &=\\sum_{i=1}&#94;{k+1} c_i&#94;k \\sum_{j=0}&#94;{i-1} C_i&#94;j(-1)&#94;{i-j}n&#94;j\\\\ &=\\sum_{i=1}&#94;{k+1} c_i&#94;k \\sum_{j=1}&#94;{i} C_i&#94;{j-1}(-1)&#94;{i-j+1}n&#94;{j-1}\\\\ &= \\delta_j&#94;{k+1} n&#94;{j-1} \\end{align} 因此 $\\delta_j&#94;{k+1}=L_{ji}c_i&#94;k=\\big[\\sum_{i=1}&#94;{k+1} \\sum_{j=1}&#94;{i} C_i&#94;{j-1}(-1)&#94;{i-j+1}\\big] c_i&#94;k$ 简写求和符号，令$i,j$求和都在$1,\\ldots,k+1$范围，但是规定$i\\geq j$，得到上三角矩阵： $$L_{ji}=(-1)&#94;{i-j+1}C_i&#94;{j-1}$$ 因此，只需要求解线性方程组$L_{ji}c_i&#94;k=\\delta_j&#94;{k+1}$即可得到$c_i&#94;k=L&#94;{-1}_{ij}\\delta_j&#94;{k+1}$ # 具体计算 利用$C_{i}&#94;j=C_{i-1}&#94;{j}+C_{i-1}&#94;{j-1}$可以进行递推运算 In [13]: from fractions import Fraction from linear_solver import solve_triangular , fractize def LMatrix ( k ): N = arange ( k + 1 ) L = diag ( N + 1 ) L [ 0 ] = 1 for i in range ( 2 , k + 1 ): L [ 1 : i , i ] = L [ 0 : i - 1 , i - 1 ] + L [ 1 : i , i - 1 ] L *= ( - 1 ) ** ( N [:, np . newaxis ] + N [ np . newaxis , :]) return fractize ( L ) def delta ( k ): d = zeros ([ k + 1 , 1 ]) d [ - 1 ] = 1 return fractize ( d ) def coef ( k ): return solve_triangular ( LMatrix ( k ), delta ( k ))[ 1 ] . flatten () def poly ( c ): def _poly ( n ): return dot ( c , cumprod ( ones_like ( c , dtype = 'int' ) * n )) return _poly In [73]: for i in range ( 6 ): print ( coef ( i )) [1] [1/2 1/2] [1/6 1/2 1/3] [0 1/4 1/2 1/4] [-1/30 0 1/3 1/2 1/5] [0 -1/12 0 5/12 1/2 1/6]","tags":"Computer","url":"https://www.peijun.me/zheng-shu-mi-de-qiu-he.html"},{"title":"Normalize color by center","text":"Also on Github Gist In [3]: from matplotlib.colors import Normalize class CenterNorm ( Normalize ): def __init__ ( self , vc = 0 , cc = 0.5 , vmin = None , vmax = None , clip = False ): ''' Args: vc value of center cc color of center ''' Normalize . __init__ ( self , vmin , vmax , clip ) assert 0 < cc < 1 , \"Central color should be in (0, 1)\" self . vc = vc self . cc = cc def __call__ ( self , value , clip = None ): dv = np . array ([ self . vc - self . vmin , self . vmax - self . vc ]) dc = np . array ([ self . cc , 1 - self . cc ]) k = 1 / max ( dv / dc ) return np . ma . masked_array (( value - self . vc ) * k + self . cc ) In [4]: def cond ( x , mask ): y = x . copy () y [ ~ mask ] = nan return y In [22]: def imshow_xyz ( ax , x , y , z , * args , ** argv ): '''x is x of xoy rhs system, i.e. j index of image y is y of xoy rhs system, i.e. -i index of image z is function of x, y, i.e. z[i, j]=f(x[i], y[j]) Image is actual image matrix, I[i, j]=z[-j, i]''' assert z . shape == ( len ( x ), len ( y )), \"Shape unmatched\" ex = np . mean ( np . diff ( x )) / 2 ey = np . mean ( np . diff ( y )) / 2 argv [ 'extent' ] = ( x [ 0 ] - ex , x [ - 1 ] + ex , y [ 0 ] - ey , y [ - 1 ] + ey ) return ax . imshow (( z . T )[:: - 1 , :], * args , ** argv ) def zgrid ( x , y , f ): z = x [:, np . newaxis ] + 1 j * y [ np . newaxis , :] return np . vectorize ( f )( z ) def xygrid ( x , y ): return x [:, np . newaxis ], y [ np . newaxis , :] In [27]: import matplotlib.gridspec as gridspec # Generate data to plot x = linspace ( - pi , pi , 100 ) y = linspace ( - pi , pi , 100 ) r = sqrt ( 2 ) * linspace ( - pi , pi , 300 ) xz , yz = xygrid ( x , y ) z = sinc ( sqrt ( xz ** 2 + yz ** 2 )) norms = [ Normalize (), CenterNorm ()] titles = [ \"Normalize\" , \"CenterNorm\" ] gs = gridspec . GridSpec ( 3 , 4 ) # Plot z = sinc(r) ax = plt . subplot ( gs [ 0 , :]) pos = cond ( r , sinc ( r ) >= 0 ) #-1e-2) neg = cond ( r , sinc ( r ) <= 0 ) #1e-2) ax . plot ( neg , sinc ( neg )) ax . plot ( pos , sinc ( pos )) ax . set_title ( r '$z=\\mathrm {sinc} (r)$' ) ax . grid () # Plot comparation between two color normalization choices gs . update ( hspace = 0.5 , wspace = 0.4 ) for i in range ( 2 ): ax = plt . subplot ( gs [ 1 :, 2 * i : 2 * ( i + 1 )]) c = imshow_xyz ( ax , x , y , z , cmap = \"RdBu_r\" , norm = norms [ i ], interpolation = 'bilinear' ); ax . set_title ( titles [ i ]) #, y=-0.3) plt . colorbar ( c , ax = ax , orientation = 'vertical' );","tags":"Computer","url":"https://www.peijun.me/normalize-color-by-center.html"},{"title":"Gnome 配置","text":"# GNONE插件 https://extensions.gnome.org/ # 必备 Dash to Panel KStatusNotifierItem/AppIndicator Support Pixel Saver # 次要 'Window Is Ready' Notification Remover by nunofarruca Bumblebee Status by dsboger gTile by scherepanov Media Player Indicator by JasonLG1979 Lunar Calendar 农历 by Nei Remove Rounded Corners by mbokil Pomodoro by kamilprusko Suspend Button by laser_b Simple net speed by bijignome # 任务栏不合并kde程序的bug 使用 https://github.com/bil-elmoussaoui/StartupWMClassFixer import glob path = '/usr/share/applications/' l = glob . glob ( path + 'org.kde.*.desktop' ) for i in l : k = i . split ( '.' )[ - 2 ] print ( k , 'org.kde. {} ' . format ( k ), k , sep = ', ' ) 运行上述代码追加内容至 database.csv ，再运行 ./fix # 使用zsh和konsole 由于gnome-terminal的 BUG#706927 的存在, 虽然可以修改gnome-terminal的shell使之执行zsh命令，但是这并无法让nautilus打开终端的时候自动跳转到当前目录。正确方法是将zsh用作登录shell( chsh -s /usr/bin/zsh )并让终端使用登录shell，然后重新登录。","tags":"Computer","url":"https://www.peijun.me/gnome-config.html"},{"title":"Windows设置","text":"压缩 7-zip pdf阅读器 Adobe Reader Master PDF Editor WPS 图像处理 点阵图 Gimp 矢量图 Inkscape 代码开发 Code::Blocks QtCreator VS Code Mingw-w64 浏览器 Firefox中文版 Chrome 编辑器 SublimeText Notepad++ 网盘dropbox, megasync 词典 Goldendict , 欧路词典","tags":"Computer","url":"https://www.peijun.me/windowsshe-zhi.html"},{"title":"描点画图","text":"# 等距描点画图 \\begin{align} \\Delta l&=\\sqrt{\\Delta x&#94;2+\\Delta y&#94;2}=C\\\\ \\Rightarrow \\Delta x&=\\frac{\\Delta l}{\\sqrt{1+y'&#94;2}} \\end{align} # 等角描点画图 \\begin{align} \\theta&=\\arctan\\frac{\\Delta y}{\\Delta x}\\\\ \\delta\\theta&=C\\\\ \\Rightarrow \\Delta x&=\\delta\\theta\\Big/\\frac{y''}{1+y'&#94;2} \\end{align} # 无穷间断点 请参见 http://stackoverflow.com/questions/10377593/how-to-drop-connecting-lines-where-the-function-is-discontinuous ) In [1]: def tannan ( x , bound = 6 ): t = tan ( x ) if abs ( t ) > bound : return nan else : return t tann = vectorize ( tannan ) x = arange ( 0 , 9 * pi / 2 , pi / 100 ) plot ( x , tann ( x ));","tags":"Computer","url":"https://www.peijun.me/draw-with-connected-dots.html"},{"title":"Network Programming","text":"This is my note of reading network programming chapter of CSAPP Book # IP(Internet Protocal) Address Generally, it is stored(Big endian) in a struct in_addr rather than a scalar value. struct in_addr { unsigned int s_addr ; /* Network byte order (big-endian) */ }; To convert endianess between local machine word and in_addr struct, we need functions #include <netinet/in.h> //Returns: value in network byte order unsigned long int htonl ( unsigned long int hostlong ); unsigned short int htons ( unsigned short int hostshort ); //Returns: value in host byte order unsigned long int ntohl ( unsigned long int netlong ); unsigned short int ntohs ( unsigned short int netshort ); Another form is ip address string, which is human readable dotted decimal string like 192.168.100.22 . To convert between A pplication string and InterNET in_addr , we have functions: #include <arpa/inet.h> //Returns: 1 if OK, 0 on error int inet_aton ( const char * cp , struct in_addr * inp ); //Returns: pointer to a dotted-decimal string char * inet_ntoa ( struct in_addr in ); DNS Host entry contains information to domain name, ip address etc together. /* DNS host entry structure */ struct hostent { char * h_name ; /* Official domain name of host */ char ** h_aliases ; /* Null-terminated array of domain names */ int h_addrtype ; /* Host address type (AF_INET) */ int h_length ; /* Length of an address, in bytes */ char ** h_addr_list ; /* Null-terminated array of in_addr structs */ }; With following functions, we can search DNS host entry by(string): domain name or dotted decimal ip address. #include <netdb.h> //Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set struct hostent * gethostbyname ( const char * name ); //Returns: non-NULL pointer if OK, NULL pointer on error with h_errno set struct hostent * gethostbyaddr ( const char * addr , int len , 0 ); # Socket Interface Socket is uniquely identified by ip address(machine) and port number(multiple sockets for same machine). A connection is uniquely identified by the socket addresses of its two endpoints. This pair of socket addresses is known as a socket pair and is denoted by the tuple (cliaddr:cliport, servaddr:servport) . protocol is also essential to specify a socket interface. We use struct sockaddr for general socks address and internet style sockaddr_in : /* Generic socket address structure (for connect, bind, and accept) */ struct sockaddr { unsigned short sa_family ; /* Protocol family */ char sa_data [ 14 ]; /* Address data. */ }; /* Internet-style socket address structure */ struct sockaddr_in { unsigned short sin_family ; /* Address family (always AF_INET) */ unsigned short sin_port ; /* Port number in network byte order */ struct in_addr sin_addr ; /* IP address in network byte order */ unsigned char sin_zero [ 8 ]; /* Pad to sizeof(struct sockaddr) */ }; The family for internet is always AF_INET . # Establish connection between server and client Headers: <sys/types.h> and <sys/socket.h> Both server and client use socket to create socket descriptor int socket ( int domain , int type , int protocol ); Server Activate sock by bind socket to a fixed sock_addr int bind ( int sockfd , struct sockaddr * my_addr , int addrlen ); Listen to the socket address int listen ( int sockfd , int backlog ); Waits until some connection request and then accept. int accept ( int listenfd , struct sockaddr * addr , int * addrlen ); //Returns: nonnegative connected descriptor if OK, −1 on error The connected file descriptor is different from the istenfd. Client Connect to sock address of server int connect ( int sockfd , struct sockaddr * serv_addr , int addrlen ); The ephermeral sock_addr=in_addr:port is generated automatically by the kernel Communication rio_writen rio_readlineb When server receives EOF, it terminates this connection and start to wait to serve new clients. # Packed helper function Pack frequently used open_clientfd and open_listenfd .","tags":"Computer","url":"https://www.peijun.me/network-programming.html"},{"title":"健壮性拟合","text":"大概等于Hough变换。 # Sub Quadratic Loss Function $$L_k(x)=\\frac{x&#94;2}{1+|x|&#94;{2-k}}, \\quad 0\\leq k\\leq 2$$$$\\lim_{x\\to0} L_k(x)=x&#94;2,\\quad \\lim_{x\\to\\infty} L_k(x)=x&#94;k$$ At $|x|&#94;{2-k}\\ll 1$, it is $x&#94;2$. That is $|x|<e&#94;{-C/(2-k)}$, where $C\\sim3$. In linear regression, we often use loss function $L_2(x)=x&#94;2/2$ which leads to linear fitting. # Gain Function For $k=0$, we can define Gain function $$G(x)=1-L_0(x)=\\frac{1}{1+x&#94;2}$$ Consider scaling factor $l$, $$G_l(x)=\\frac{1}{1+(x/l)&#94;2}$$ For some parameter $\\lambda$, calculate the gain $\\Gamma(\\lambda)=\\sum_i G_l(x_i)$. The optimized gain means best estimation. # Consider derivative The derivative should be parallel to its side. Otherwise it will be abandoned. # 例子——正比例函数 此时Hough空间只有一维 In [1]: N = 100 A = 10 def gain ( ds ): return np . sum ( exp ( - ( ds * ds ) / 2 )) def distance ( lamb , pts ): k , b = lamb x , y = pts dy = y - ( k * x + b ) return gain ( dy / A ) x0 = arange ( - N , N + 1 ) x = concatenate ([ x0 , x0 ]) y = concatenate ([ x0 , - x0 ]) + A * randn ( 4 * N + 2 ) l = array ([ distance (( i , 0 ), ( x , y )) for i in linspace ( - 2 , 2 , 20 * N + 1 )]) plot ( linspace ( - 2 , 2 , 20 * N + 1 ), l ) grid (); In [2]: plot ( x , y , '.' ); grid ();","tags":"ProbStat","url":"https://www.peijun.me/robust-fitting.html"},{"title":"分钱问题","text":"设总钱数为$M$，总人数为$N$，第$i$个人的钱数为$m_i$。 我们需要随机抽满足以下约束的格点$(m_1,\\ldots, m_N)$: $$\\sum_{i=1}&#94;N m_i = M, m_i\\ge 0$$ $m_i$为第$i$个人的钱数， 求第$i$个人的钱数$m_i$取值的概率分布 求对某固定钱数$m$，抽到这个钱数的人的数量$n_m=\\sum (m_i = m)$ # 每个人的钱数分布 首先，定义归一化钱数$x_i=m_i/M$。如果我们取极限近似，假设钱数足够大即$M\\gg n$，那么$x_i$在$(0,1)$区间上的取值是准连续的。假设$M=10000,n=100$，那么这个要求显然是能够满足的。 考虑用隔板法生成$x_i$：在$[0,1]$区间 随机均匀 撒$n-1$个点$\\xi_i$将区间分成$n$份，每一份的长度从左到右依次是各个玩家的归一化钱数$x_i$。 由于所有玩家是互相 等价 的，我们只需要分析一号玩家的钱数分布。一号玩家的归一化钱数$x$等于最小的分隔点的值$\\min_i[\\xi_i]$。它的累积分布函数为： $$F(x_0)=P(x< x_0) =1-(1-x_0)&#94;{n-1}$$ 求导得概率密度 $$f(x)=(n-1)(1-x)&#94;{n-2}$$ 通过变量代换$m=Mx$，我们得出$m$的概率密度是$g(m)=f(Mx)/M$，因此$g(0)\\approx (n-1)/M$。估算$m=0$的占有数大概是$n(n-1)/M$。 以上分析思路，即使不做准连续近似依然适用：先计算累积分布函数 $P(x < m/M)$，再通过差分计算单点的概率 $$P\\left(x=\\frac{m}{M}\\right)=P\\left(x<\\frac{m+1}{M}\\right)-P\\left(x<\\frac{m}{M}\\right)$$ 某人钱数小于平均钱数的概率： \\begin{align} P(x< 1/n) &=F(1/n)\\\\ &=1-(1-1/n)&#94;{n-1}\\\\ &\\approx 1-1/\\mathrm{e}\\\\ &=0.6321205588285577 \\end{align} # 进一步分析 设$k=nx\\sim 1$, 则$f(k)=\\dfrac{n-1}{n}(1-k/n)&#94;{n-2}\\approx \\mathrm{e}&#94;{-k}$近似为指数分布 # 某个钱数的占有数分布??? 由于各个钱数彼此不独立，所以不好直接计算。","tags":"ProbStat","url":"https://www.peijun.me/money-distribution.html"},{"title":"Processing the capillary force video","text":"# Frames Export Get frame rate information videoname = T-L \\ _ \\ 1 -50 \\ tip-tip.avi ffmpeg -i $videoname 2 > & 1 | grep -o '[0-9]\\+ fps' The output is 30 fps ffmpeg -i $videoname -r 30 output_%04d.png # Edges Detection The Canny edge detector is used in this step. In [1]: from capillary import edge , fitting , display In [8]: from importlib import reload reload ( fitting ); reload ( display ); In [3]: # Read the 72nd Frame im = edge . R [ 0 ]( 72 ) # Extract coordinates of edges points = edge . G [ 0 ]( 72 ) x , y = points In [4]: imshow ( im ); In [5]: plot ( x , y , 'o' , markersize = 1 ) xlabel ( 'Row/px' ) ylabel ( 'Column/px' ) axis ( 'equal' ); title ( 'Detected Edges' ); # Use Major Axis to split particles Theoretically, if a particle has symmetry for rotate $2\\pi/3$, the moment of inertia $I_0$ is independent on the direction of axis as long as it passes the centroid. More general, for arbitrary axis, we can use parallel axis theorem to find $I=I_0+md&#94;2$. Lemma For two particles the major axis $\\hat n$ of least moment of inertia must go through both centroid. So the major axis is used to estimate the line connecting both centroid. The information of another major axis $\\hat m$ with greatest moment of inertia can also be used to estimate the separation $r$ between the centroids. As $I_n\\approx 2I_0, I_m\\approx 2\\left[I_0+m\\left(\\dfrac{r}{2}\\right)&#94;2\\right]$, we have $$\\frac{r}{2}\\approx\\sqrt{\\frac{I_M-I_m}{2m}}$$ If the two particles are separated far enough, we can separate them into two particles based on the major axis $\\hat n$ and centroids $O$ of two particles by criteria $$\\mathrm{sgn}\\left[(\\vec r_i-\\vec r_O)\\cdot \\hat n\\right]$$ In [6]: display . show_split ( points ) display . show_axis ( points ) axis ( 'equal' ); title ( 'Partition of Particles' ); When two particles are too close, the partition and the estimation of centroids may be inaccurate. But it will nevertheless give a good starting point. We will discuss this later. # Use Triangles to Fit Particles Triangles are used to Fit Particles. Fitting parameters of Triangle $T$ are centroid position $C(x, y)$ and angle location $(\\rho, \\theta)$ with respect to centroid $C$. The idea is like https://link.springer.com/article/10.1007%2FBF00939613 : $$\\mathrm{minimize}\\left[\\sum_i|\\vec r_i-\\vec r_C|&#94;2d&#94;2(\\vec r_i, T)\\right]$$ The initial estimation of centroid and angle should be given to ensure a good global minimum. In [9]: display . show_frame ( points ) axis ( 'equal' ); title ( 'Hulls and Centroids' ); # Deal with close case In [10]: pts = edge . G [ 2 ]( 1196 ) pos , neg = fitting . split ( pts ) display . show_split ( pos , neg ) display . show_axis ( pts ) axis ( 'equal' ); title ( 'Naive partition using major axes fails when close' ); /home/zpj/code/capillary/capillary/fitting.py:46: UserWarning: Particles are too close, separation may fail! warnings.warn(\"Particles are too close, separation may fail!\") If we fit triangles on the wrong split, the result is still good. In [11]: display . show_frame ( pts ) axis ( 'equal' ); title ( 'Hulls and Centroids' ); This is a good starting point for our iteration: Find a new split $S'$ based on the fitting triangles $T$. Find new fitting triangle $T'$ based on $S'$ The $T\\to S$ is given in last section. For the $S\\to T$ step, the criteria for each point are: $$d(\\vec r_i, T)=\\mathrm{distance}(\\vec r_i, \\mathrm{triangle}),$$ which is defined to be positive if point is inside a triangle and negative if outside. A point $P$ belongs to $T_1$ iff $$d(P, T_1) < d(P, T_2)+\\Delta$$ The $\\Delta$ is controlling the contiguous points which belongs to both. we are setting it to be 1 pixel. Even one iteration could give fair result: In [12]: display . show_frame ( pts , iterate = 1 ) axis ( 'equal' ); title ( 'Hulls and Centroids' ); Three iterations makes it perfect: In [13]: display . show_frame ( pts , iterate = 3 ) axis ( 'equal' ); title ( 'Hulls and Centroids' ); # Deal with Touching thick border? Solved by Fix the radius of triangle to 30","tags":"Computer","url":"https://www.peijun.me/processing-the-capillary-force-video.html"},{"title":"Covariant Linear Fit","text":"# Aim Minimize $$\\sum_i \\mathrm{distance}&#94;2(\\vec r_i, \\mathrm{line})=\\sum_i (\\vec r_i\\cdot \\hat n-\\rho)&#94;2$$ for line $\\vec r\\cdot \\hat n-\\rho=0$. It is equivalent to The principle axis with least moment of inertia The eigenvector with largest eigenval for the covariance matrix # When is it useful? Only when fitting geometrical dots, which should be rotational invariant. Ordinary linear fit does not has such an invariance. For most common case, the $x, y$ has different dimension. As an example, the relation between weight $w$ and height $h$ is not invariant under rotation, because there is no mixture between them. In this condition, we need invariance of scaling for any axis. # Code In [1]: def clinearfit ( lx , ly ): x , y = mean ( lx ), mean ( ly ) val , vec = eigh ( cov ( lx , ly )) idx = val . argsort () vec = vec [:, idx ] return array ([ x , y ]), vec [ 1 ] In [118]: def fitline ( lx , ly , extend = 0.1 ): center , direct = clinearfit ( lx , ly ) assert ( direct [ 0 ]) k = direct [ 1 ] / direct [ 0 ] dr = transpose ( vstack ([ lx , ly ])) - center ds = dot ( dr , direct ) def f ( x ): return k * ( x - center [ 0 ]) + center [ 1 ] x = sort ( array ([ max ( ds ), min ( ds )]) * direct [ 0 ]) + center [ 0 ] d = x [ 1 ] - x [ 0 ] x [ 0 ] -= extend * d x [ 1 ] += extend * d return f , x # Probabilistic The likelihood is ? In [126]: lx = [ 0 , 0.6 , 0.4 ] ly = [ 0 , 0.4 , 0.6 ] f , x = fitline ( lx , ly ) plot ( lx , ly , 'o' ) axis ( 'equal' ) plot ( x , f ( x ));","tags":"Computer","url":"https://www.peijun.me/covariant-linear-fit.html"},{"title":"Convert str(array) back to numpy array","text":"If we print a numpy array, which actually use str() , we will find it almost irreversible. In [5]: l = arange ( 16 ) . reshape ( 4 , 4 ) print ( 'l is printed as: \\n ' , l ) l is printed as: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] Use print() will fallback to str() , so str() is not the correct way. repr() .tolist() In [77]: print ( 'Array Form: \\n ' , repr ( l )) print ( 'List form: \\n ' , l . tolist ()) Array Form: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) List form: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]] # Recover your fault If you really need to convert the array string separated by spaces into python, here is a remedy str2array() : In [21]: import re import ast def array2str ( arr , precision = None ): s = array_str ( arr , precision = precision ) return s . replace ( ' \\n ' , ',' ) def str2array ( s ): # Remove space after [ s = re . sub ( '\\[ +' , '[' , s . strip ()) # Replace commas and spaces s = re . sub ( '[,\\s]+' , ', ' , s ) return array ( ast . literal_eval ( s )) In [20]: str2array ( array2str ( sqrt ( l ), 3 )) Out[20]: array([[ 0. , 1. , 1.414, 1.732], [ 2. , 2.236, 2.449, 2.646], [ 2.828, 3. , 3.162, 3.317], [ 3.464, 3.606, 3.742, 3.873]])","tags":"Physics","url":"https://www.peijun.me/convert-strarray-back-to-numpy-array.html"},{"title":"又双叒叕——汉字的堆叠","text":"你可能会有这样的问题： 类似\"又双叒叕\"这样子的汉字有多少？ 本文就是对此问题的分析。 # 数据来源 原始数据来源于参考维基百科条目 二叠字 、 三叠字 、 四叠字 。 格式化后的的数据，参见 源文件 。 本页面含有 Unihan 新版用字。有关字符可能会错误显示，详见Unicode扩展汉字。 # 数据分析 使用pandas进行数据表连接查询，可以找到同时具有多种叠字的字。 In [2]: import pandas as pd from functools import reduce In [3]: data = [ pd . read_table ( ' %d .txt' % i ) for i in range ( 2 , 5 )] In [4]: def merge_out ( x , y ): return pd . merge ( x , y , on = '单' , how = 'outer' ) jointed = reduce ( merge_out , data ) # 具有多种堆叠方式的字 # 具有二三四堆叠 In [5]: jointed . dropna () Out[5]: 单 双 三 四 1 一 二 三 亖 16 人 仌 众 𠈌 17 人 仌 㐺 𠈌 18 人 从 众 𠈌 19 人 从 㐺 𠈌 36 厶 厸 厽 𠫬 37 厶 㕕 厽 𠫬 38 又 双 叒 叕 39 又 㕛 叒 叕 40 口 吅 品 㗊 41 口 吅 𠱠 㗊 42 口 吕 品 㗊 43 口 吕 𠱠 㗊 53 土 圭 垚 㙓 65 屮 艸 芔 茻 87 日 昌 晶 𣊫 88 日 昌 晿 𣊫 89 日 昍 晶 𣊫 90 日 昍 晿 𣊫 95 木 林 森 𣛧 96 木 林 𣓏 𣛧 97 朿 棗 𣝯 𣡍 98 朿 棘 𣝯 𣡍 100 果 𣛕 𣡗 𣡾 106 水 沝 淼 㵘 107 水 沝 㴇 㵘 109 火 炎 焱 燚 110 火 炎 㷋 燚 111 火 炏 焱 燚 112 火 炏 㷋 燚 115 牛 牪 犇 𤛭 122 田 畕 畾 𤳳 129 石 砳 磊 𥗉 136 老 䎜 𦓈 𦓋 145 言 誩 譶 𧮦 150 車 䡛 轟 𨏿 153 金 鍂 鑫 𨰻 161 風 䬕 飍 𩙡 166 魚 䲆 鱻 䲜 167 魚 𩺰 鱻 䲜 169 龍 龖 龘 𪚥 # 具有二三四堆叠的单字 In [30]: def single_word ( table ): return '' . join ( table . dropna ()[ '单' ] . drop_duplicates () . tolist ()) In [31]: single_word ( jointed ) Out[31]: '一人厶又口土屮日木朿果水火牛田石老言車金風魚龍' # 金木水火土 金 鍂 鑫 𨰻 木 林 森 𣛧 𣡽 土 圭 垚 㙓 水 沝 淼 㵘 火 炏 焱 燚 # 只具有二三堆叠的单字 In [34]: S3 = set ( single_word ( jointed [[ '单' , '双' , '三' ]])) S4 = set ( single_word ( jointed )) '' . join ( S3 - S4 ) Out[34]: '女香馬力虫户瓜欠舌ㄑ子㔾太耳手毛隹飞生犬山白面大心吉目弓' # 双叠字 In [6]: jointed [[ '单' , '双' ]] . drop_duplicates () . dropna () Out[6]: 单 双 0 ㄑ 巜 1 一 二 2 卄 卌 3 串 丳 4 乂 爻 5 尹 𡱉 6 𠂢 𠃄 7 乙 𠃐 8 九 𠃙 9 习 羽 10 𠄌 𠃏 11 予 㐨 12 二 亖 13 井 㐩 14 亦 𠅯 15 享 㝇 16 人 仌 18 人 从 20 來 𠐇 21 來 㯤 22 倉 𠑐 23 僉 𠑲 24 兀 𠀘 25 兄 㒭 26 先 兟 27 入 𠓜 28 几 𠘬 29 凡 𠘻 30 力 𠠴 31 匕 比 ... ... ... 138 臣 𦣦 139 至 臸 140 舌 𦧚 141 虎 虤 142 虫 䖵 144 見 覞 145 言 誩 146 豕 豩 147 貝 賏 148 走 𧼂 149 足 踀 150 車 䡛 151 辛 辡 152 邑 䣈 153 金 鍂 154 隹 雔 156 隻 䨇 157 面 𩈲 158 面 𩈳 159 音 䪭 160 竟 竸 161 風 䬕 162 飞 𠃧 163 香 𩡌 164 香 𩡐 165 馬 騳 166 魚 䲆 167 魚 𩺰 168 齒 𪙹 169 龍 龖 157 rows × 2 columns # 三叠字 In [7]: jointed [[ '单' , '三' ]] . drop_duplicates () . dropna () Out[7]: 单 三 0 ㄑ 巛 1 一 三 16 人 众 17 人 㐺 30 力 劦 35 㔾 𠨕 36 厶 厽 38 又 叒 40 口 品 41 口 𠱠 48 吉 嚞 53 土 垚 57 大 𡘙 58 太 𡙒 61 女 姦 62 子 孨 63 子 𡥦 65 屮 芔 66 山 𡷈 75 弓 𢏝 79 心 惢 81 户 𢩕 83 手 掱 87 日 晶 88 日 晿 95 木 森 96 木 𣓏 97 朿 𣝯 100 果 𣡗 102 欠 𣣓 ... ... ... 184 妥 𤕇 185 宜 𡬐 186 客 𡬚 187 寒 𡬜 188 小 尛 189 巛 𡿭 190 市 𢅈 191 文 𣁕 192 林 𣡕 193 止 歮 194 泉 灥 195 甲 𤳅 196 盖 𥃣 197 直 矗 198 眉 𡳻 199 秦 䆐 200 空 𥩌 201 竹 𥴒 202 羊 羴 203 羗 𦏱 204 興 𦧅 205 舍 𠑰 206 若 䖃 207 贝 贔 208 门 𨷮 209 雲 䨺 210 雷 靐 211 頁 𩖏 212 马 𩧢 213 鹿 麤 104 rows × 2 columns # 四叠字 In [8]: jointed [[ '单' , '四' ]] . drop_duplicates () . dropna () Out[8]: 单 四 1 一 亖 4 乂 㸚 16 人 𠈌 26 先 𠓙 31 匕 𣬅 36 厶 𠫬 38 又 叕 40 口 㗊 53 土 㙓 65 屮 茻 87 日 𣊫 94 月 朤 95 木 𣛧 97 朿 𣡍 100 果 𣡾 106 水 㵘 109 火 燚 115 牛 𤛭 122 田 𤳳 129 石 𥗉 136 老 𦓋 145 言 𧮦 150 車 𨏿 153 金 𨰻 161 風 𩙡 166 魚 䲜 169 龍 𪚥 183 天 𡚌 188 小 𡮐 192 林 𣡽 194 泉 𤆁 195 甲 𤳵 201 竹 𥷹 204 興 𠔻 209 雲 𩇔 210 雷 䨻 214 且 𠁠 215 丶 灬 216 囚 𡈶 217 困 𡈹 218 工 㠭 219 春 𣌠 220 曰 𣊭 221 門 𨷾","tags":"ProbStat","url":"https://www.peijun.me/repeating-hanzi.html"},{"title":"Reduced density matrix and partial trace","text":"# Reduced density matrix Suppose we have two quantum systems $a, b$, with dimension $N_a, N_b$ respectively. Then the Hilbert space of $a+b$ is of dimension $N=N_aN_b$. Suppose we have a density matrix $$\\hat\\rho=\\sum_{i,j}\\rho_{ij}\\lvert i\\rangle \\langle j\\rvert=\\sum_{i,j,k,l}\\rho_{ijkl}\\lvert i\\rangle_a\\lvert j\\rangle_b \\langle k\\rvert_a\\langle l\\rvert_b$$ Then the reduced density matrix of $a$ is defined as $$\\hat\\rho_a=\\mathrm{tr}_b\\hat\\rho=\\sum_i \\langle i\\rvert_b\\hat\\rho\\lvert i\\rangle_b$$ i.e. reduced density matrix problem is equivalent to partial trace problem. # Tensor In fact, if we take $\\hat\\rho$ as a 4-tensor $\\rho_{ijkl}$, then the reduced density matrix is $$\\rho&#94;{(a)}_{ij}=\\delta&#94;{\\mu\\nu}\\rho_{i\\mu k\\nu}$$ For simple density matrix $\\rho=\\lvert \\psi\\rangle \\langle \\psi\\rvert$, the reduced matrix is $$\\rho&#94;{(a)}_{ik}=\\delta&#94;{jl}\\rho_{ijkl}=\\delta&#94;{jl}\\psi_{ij}\\psi&#94;+_{lk}=\\sum_i |\\langle i_b\\lvert \\psi\\rangle|&#94;2=[\\psi\\psi&#94;+]_{ik}$$ Here we are taking $\\psi$ as an $N_a\\times N_b$ matrix. For general case, if we find decomposition $$\\rho=\\sum_c \\lambda_c\\lvert \\psi_c\\rangle \\langle \\psi_c\\rvert,\\quad \\sum_c \\lambda_c=1$$ then we have $$\\rho&#94;{(a)}_{ik}=\\left[\\sum_c\\lambda_c\\psi_c\\psi&#94;+_c\\right]_{ik}$$ # Partial trace # Computation of partial trace by hand $\\rho$ can be regarded as $N_a\\times N_a$ block matrices $A_{ij}$ with size $N_b\\times N_b$ for each $A_{ij}$. Partial trace over $a$ is like summation over block matrices in the diagonal: $\\sum_i A_{ii}$ Partial trace over $b$ is like tracing over all $A_{ij}$ block matrices in the diagonal: $A_{ij}\\rightarrow \\mathrm{tr}(A_{ij})$ # Compution of partial trace in numpy It seems that there is no partial trace function in numpy, but there is trace(contraction) function for ndarray(tensors). As an example, suppose we have an $N\\times N$ density matrix $\\rho$: In [1]: n1 , n2 , n = 2 , 2 , 4 A = arange ( 4 )[ newaxis , :] rho = ( 1 + A + A . transpose ()) rho = rho / trace ( rho ) print ( rho ) [[ 0.0625 0.125 0.1875 0.25 ] [ 0.125 0.1875 0.25 0.3125] [ 0.1875 0.25 0.3125 0.375 ] [ 0.25 0.3125 0.375 0.4375]] Then, we can recast it to a tensor with more axes by .reshape() . In [3]: rho_tensor = rho . reshape ([ n1 , n2 , n1 , n2 ]); The last step is contraction: Partial trace over $a$ is contraction over axis $0, 2$, which produces $\\rho_b$ Partial trace over $b$ is contraction over axis $1, 3$, which produces $\\rho_a$ In [4]: trace ( rho_tensor , axis1 = 1 , axis2 = 3 ) #rho_a Out[4]: array([[ 0.25, 0.5 ], [ 0.5 , 0.75]]) In [5]: trace ( rho_tensor , axis1 = 0 , axis2 = 2 ) #rho_b Out[5]: array([[ 0.375, 0.5 ], [ 0.5 , 0.625]])","tags":"Physics","url":"https://www.peijun.me/reduced-density-matrix-and-partial-trace.html"},{"title":"对换钱悖论的贝叶斯分析","text":"# 问题的提出 现在邀请你和一位路人甲来做一个游戏：我拿出两个信封分别递给你们，并告诉你 们一个装着的钱是另一个的两倍（但不知道哪个多哪个少）。你们有一次互相交换 的机会，想交换吗？然后打开信封，看一下自己拿到的钱数（但不要让对方知道），现在还想交换吗？ —— 引自0x01.me 这段话似乎可以引出一个悖论： 换钱后，有一半的概率钱变成 原来 的两倍，一半概率钱减少到 原来 的一半，因此期望收益是大于零的。然而另一方面，两份钱显然是对称的，因此不可能总是由换钱带来收益。 下面我们用贝叶斯来分析这个悖论。 # 贝叶斯分析 假设发钱时，钱少的那个钱数取$y$的先验分布为$\\rho(y)$。设某人抽到了钱数是$x$，则此时$y$的取值可以是$x$或者$x/2$。 参数$y$下$x$的分布是 $$p(x|y)=\\frac{\\delta(x-2y)+\\delta(x-y)}{2}$$ 因此$x$本身的发生概率： \\begin{align} p(x)&=\\int p(x|y)\\rho(y)dy\\\\ &=\\frac{1}{2}\\int [\\delta(x-2y)+\\delta(x-y)]\\rho(y)dy\\\\ &=\\frac{\\rho(x)}{2}+\\frac{\\rho(x/2)}{4} \\end{align} 已知$x$的信息的情况下，后验分布是： \\begin{align} p(y|x)&=\\frac{p(x|y)\\rho(y)}{p(x)}\\\\ &=\\frac{\\delta(x-2y)+\\delta(x-y)}{2p(x)}\\rho(y)\\\\ &=\\frac{\\rho(y)\\delta(y-x/2)/2+\\rho(y)\\delta(x-y)}{2p(x)}\\\\ &=\\frac{\\rho(x/2)}{4p(x)}\\delta(y-x/2)+\\frac{\\rho(x)}{2p(x)}\\delta(y-x) \\end{align} 因此离散的后验概率是 \\begin{align} p(y=x|x)&=\\frac{\\rho(x)}{2p(x)}\\\\ p(y=x/2|x)&=\\frac{\\rho(x/2)}{4p(x)} \\end{align} 换钱后钱增量$\\Delta x$， \\begin{align} \\mathrm{E}[\\Delta x]&=xp(y=x|x)-\\frac{x}{2}p(y=x/2|x)\\\\ &=\\frac{x\\rho(x)}{2p(x)}-\\frac{x\\rho(x/2)}{8p(x)} \\end{align} # 无脑交换的收益期望值 由于世界财富值有限，我们可以假定$\\rho$是有界的，亦即 $\\exists A$使得对于$\\forall x>A$，满足$\\rho(x)\\equiv 0$ 那么 $$\\begin{align}\\overline{\\mathrm{E}[\\Delta x]}&=\\int_0&#94;{2A} p(x)\\mathrm{E}[\\Delta x]dx\\\\ &=\\int_0&#94;{2A} \\frac{x\\rho(x)}{2}-\\frac{x\\rho(x/2)}{8}dx\\\\ &=\\int_0&#94;{2A} \\frac{x\\rho(x)}{2}-\\int_0&#94;A\\frac{t\\rho(t)}{2}dt,\\quad t\\stackrel{def}{=}x/2\\\\ &=0 \\end{align}$$ 所以无脑交换在合理假设下无平均收益。","tags":"ProbStat","url":"https://www.peijun.me/bayesian-analysis-for-exchanging-money.html"},{"title":"Blogging with Jupyter and Pelican","text":"ipynb2pelican is used to provide jupyter ipynb support in pelican. This my blog source code repo: https://github.com/peijunz/peijunz.github.io/tree/src , and .travis.yml # Installation and Setup Jupyter , and what is jupyter notebook Pelican , a pythonic blog system supports markdown, rst, asciidoc, ipynb etc. ipynb2pelican , a plugin enables ipynb support by metacell ghp-import is needed for github page import. It can be installed by pip Bootstrap3 Theme # Setup pelican Simply run command pelican-quickstart , and you will get the welcome information, and asked some questions. For the following questions, you probably do not want to use default setting: > What is your time zone? [Europe/Paris] US/Eastern > Do you want to upload your website using GitHub Pages? (y/N) Y > Is this your personal page (username.github.io)? (y/N) y # Setup navigation by TOC This part depends on pelican-toc plugin and applies to bootstrap3 theme. It is updated to my fork of pelican-theme repo We can add the following code in article.html before {{ article.content }} to implement the navigation(Table of contents). {% if article.toc %} <div class=\"panel panel-default\"> <div class=\"panel-heading\">Table of Contents</div> <div class=\"panel-body\"> {{article.toc}} </div> </div> {% endif %} # Remove paragraph indicator inside navigation Look for the first line below, and add the second line in : content.toc = tree_soup.decode(formatter='html') content.toc = content.toc.replace(' ', '') # Setup Git and Github Pages Set up your Github Pages Head over to GitHub and create a new repository named username .github.io, where username is your username (or organization name) on GitHub. Change directory to the blog folder containing Makefile , pelicanconf.py etc. Init git repo by git init Create a branch src by git checkout -b src . This will be your working folder Add a remote repo related to your github page and push the src to github Now you can push webpage to master branch by make github , which includes ghp-import commands. # Write your post in jupyter notebook! There is a official tutorial on how to write a MarkDown/reStructuredText article with metadata ipynb2pelican defined a metadata format here: https://github.com/peijunz/ipynb2pelican#metacell # Make your contents Generate Site make html Test site at localhost make serve Then, go to address localhost:8000 in your browser. Generate + test make devserver It will automatically regenerate html instantly after your change of the src. Deploy to github page make github # Tweaks # Extra \\n in indented code block For indent style code block Some text before This is a indented code block This is another line Some text after the block with an empty line between them It will produce Some text before This is a indented code block This is another line Some text after the block with an empty line between them So you should stick to ``` style code block or remove empty lines after indented code block. This is a bug of nbconvert # extra directory This supports an extra directory in the same directory of your Makefile , which can be used to place static files to root directory of your sites, such as: CNAME file for github page local mathjax for local preview of your site. Link local mathjax to the extra directory by ln -s MATHJAX_PATH extra/static Use MATHJAX_CDN=\"/mathjax/MathJax.js\" in pelicanconf.py Use MATHJAX_CDN=None in pelicanconf.py In html and publish section of Makefile , simply add: if test -d $( BASEDIR ) /extra ; then cp -r $( BASEDIR ) /extra/* $( OUTPUTDIR ) / ; fi # Cache Enable cache for local build but not for publishing: pelicanconf.py CACHE_CONTENT = True LOAD_CONTENT_CACHE = True publishconf.py CACHE_CONTENT = False LOAD_CONTENT_CACHE = False","tags":"Computer","url":"https://www.peijun.me/blogging-with-jupyter-and-pelican.html"},{"title":"ipynb2pelican Plugin released!","text":"Yet another Pelican Plugin for blogging with Jupyter Notebooks using MetaCell to store metadata. Thanks to super cow power of python, we can finally publish ipynb easily! Below is the README.md from the project at this time. The ipynb2pelican plugin provides markup for Jupyter/IPython notebooks in pelican, so .ipynb files are recognized as a valid filetype for an article. # MetaCell The project is inspired by pelican-ipynb , but do things the other way: MetaCell (i.e. Metadata Cell). With MetaCell, there is NO need to create another metadata file, or edit ipynb externally. Everything is inside Jupyter Notebook! Exact Idea of MetaCell is: All and Only Metadata should be stored at the first Cell of ipynb Writing a MetaCell is as simple as writting metadata in markdown file. # This is title + date: 2020-02-22 + tags: [hello, world] Thanks to the markdown capability of ipynb, MetaCell will be shown like the following: # This is title date: 2020-02-22 tags: [hello, world] So, MetaCell itself will even enhance the readability of your notebooks! Hint: In jupyter notebook, press Esc+M will switch selected cell to markdown mode. # Overview The plugin is simple: The CSS of jupyter will not be taken into outputs The summary will be generated by Pelican But it is still powerful and extensible: Math Support A Solution for metadata Several configurable preprocessors provided Metadata Extraction SubCell Selection Ignore cells with #ignore tag Empty Cell Removal You can change preprocess.py and define your own preprocessors # Preprocessors # Metadata Extraction As we stated, All and Only Metadata should be stored at the first Cell of ipynb. If there is non-metadata content found, it will raise an exception. After the extraction of metadata, the MetaCell will be removed , as we have extracted all the information. # SubCells Selection The Subcells preprocessor is executed after Metadata preprocessor (Th MetaCell it self will be removed by Metadata preprocessor), so zeroth cell is the first cell after MetaCell . The start and end should be written in the MetaCell like: # This is title + date: 2020-02-22 + tags: [hello, world] + subcells: [5, -1] The value will be evaluated by start, end = ast.literal_eval(value) . And then cells are sliced by cells[start:end] . Hint: If you want end to be infinity, use None # #ignore Tag You can include an #ignore comment at the beginning of a cell of the Jupyter notebook to ignore it, removing it from the post content. Note it is more strict than #ignore tag in pelican-ipynb. The purpose is to prevent kicking normal contents out of post content. # Remove Empty Cells Remove trivial cells without visible characters using regular expression \\S # Installation # Dependency Currently works for python3 only pelican nbconvert jupyter ipython Download this repo and put all the .py files it into an ipynb directory into your plugins directory. In the pelicanconf.py MARKUP = ( 'md' , 'ipynb' ) PLUGIN_PATH = 'pelican-plugins' PLUGINS = [ 'pelican-plugins' ] # Options Option Default Meaning IPYNB_REMOVE_EMPTY True Remove Empty Cells, True by default IPYNB_IGNORE True Remove cells with #ignore tag at the beginning IPYNB_SUBCELLS True Only preserve Subcells specified by subcells: [begin, end) metadata # TODO Fix the inperfect environment support? Syntax highlight for markdown cells # Acknowledgement Thanks to pelican-ipynb ! From reading the code of the project, I have learned how to write a similiar plugin with my own ideas.","tags":"Computer","url":"https://www.peijun.me/ipynb-plugin.html"},{"title":"Next Permutation","text":"Aim : Find a larger permutation We must move a larger number forward. So we should go from the tail to find any new number is smaller than a number behind it. If no next permutation, reverse list. In [ ]: def next_permutation ( l ): '''Give out the next permutation of list l >>> next_permutation([2, 1, 3]) [2, 3, 1] >>> next_permutation([3, 2, 1]) [1, 2, 3] ''' n = len ( l ) for i in range ( n - 2 , - 1 , - 1 ): if l [ i ] < l [ i + 1 ]: for j in range ( n - 1 , i , - 1 ): if l [ j ] > l [ i ]: l [ j ], l [ i ] = l [ i ], l [ j ] l [ i + 1 :] = l [ n - 1 : i : - 1 ] return l return l [:: - 1 ] # Peumutation Loop until reoccurance In [ ]: def permu_circ ( l ): '''Loop over permutations until goes back to initial permutation given by list l >>> for k in permu_circ([2,1,3]): print(k) [2, 1, 3] [2, 3, 1] [3, 1, 2] [3, 2, 1] [1, 2, 3] [1, 3, 2] ''' assert ( isinstance ( l , list )) ini = l . copy () while True : yield l l = next_permutation ( l ) if l == ini : break return # Permutation of 1 to n It is inefficient for all permutations in this way In [3]: def permu_num ( n ): '''Permutations over 1, 2,..., n >>> for k in permu_num(3): print(k) [1, 2, 3] [1, 3, 2] [2, 1, 3] [2, 3, 1] [3, 1, 2] [3, 2, 1] ''' return permu_circ ( list ( range ( 1 , n + 1 ))) if __name__ == '__main__' : import doctest doctest . testmod ()","tags":"Puzzles","url":"https://www.peijun.me/next-permutation.html"},{"title":"Notes for Chapter 4. Gaussian Models","text":"$\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\newcommand{\\pfr}[2]{\\frac{\\pp #1}{\\pp #2}} % Partial derivative \\newcommand{\\inv}[1]{#1&#94;{-1}} % Inverse Matrix \\newcommand{\\invt}[1]{#1&#94;{-T}} % Inverse Transposed Matrix \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS} \\DeclareMathOperator{\\Beta}{B} \\newcommand{\\nl}{\\\\&\\phantom{={}}} $ # Matrix derivative We are using Einstein's summation rule below: sum over repeating index. It is useful in evaluating matrix/tensor. \\begin{align} \\delta |A|&=|A+\\delta A|-|A|\\\\ &=|D+\\inv U\\delta A\\inv V|-|A|\\\\ &=|D||I+\\inv A\\delta A|-|A|\\\\ &=|A|\\tr(\\inv A\\delta A) \\end{align} Using \\begin{align} \\pfr{|A|}{A}&=e_ie_j&#94;T\\pfr{|A|}{A_{ij}}\\\\ &=e_ie_j&#94;T|A|\\tr(\\inv A\\pfr{A}{A_{ij}})\\\\ &=e_ie_j&#94;T|A|\\tr(\\inv Ae_ie_j)\\\\ &=e_ie_j&#94;T|A|\\inv A_{ji}\\\\ &=|A|\\invt A \\end{align}$$\\pfr{A_{ij}B_{ji}}{A}=e_me_n&#94;T\\pfr{A_{ij}B_{ji}}{A_{mn}}=B_{nm}e_me_n&#94;T=B&#94;T$$$$\\pfr{a&#94;TAa}{a}=e_m\\pfr{a_iA_{ij}a_j}{a_m}=e_m(A_{mj}a_j+a_i A_{im})=(A+A&#94;T)a$$$$\\pfr{a&#94;TAa}{A}=e_me_n&#94;T\\pfr{a_iA_{ij}a_j}{A_{mn}}=aa&#94;T$$$$\\pfr{\\tr AB}{A}=e_me_n&#94;T\\pfr{A_{ij}B_{ji}}{A_{mn}}=e_me_n&#94;TB_{nm}=B&#94;T$$ Using these relationships, it is straightforward to MLE # Sigmoid model $$\\exp\\left[\\frac{(x-\\mu)&#94;2}{2\\sigma&#94;2}\\right]=\\exp\\left(\\frac{x&#94;2+\\mu&#94;2-2\\mu x}{2\\sigma&#94;2}\\right)=Af(x)\\exp(\\mu x/\\sigma&#94;2)$$$$\\frac{A\\exp(ax)}{A\\exp(ax)+B\\exp(bx)}=\\frac{1}{1+\\exp(Dx+C)}$$ # Maximal Entropy of Gaussian Dist from Variation Consider entropy \\begin{align} S&=-\\int p\\ln p \\dd V-a\\left(\\int p\\dd V-1\\right)-\\sum_i b_i\\left(\\int x_ip\\dd V\\right)\\nl-\\sum_{i,j} C_{ij}\\left(\\int x_ix_jp\\dd V-\\Sigma_{ij}\\right)\\\\ &=-\\int (\\ln p+a+b&#94;Tx+x&#94;TCx)p \\dd V+a+\\tr(C\\Sigma)\\\\ &=-\\int L(p,x)\\dd x+a+c \\end{align} The Euler-Lagrange equation is \\begin{align} \\pfr{L}{p}&=1+a+b&#94;Tx+x&#94;TCx+\\ln p=0\\\\ \\Rightarrow p(x)&=\\exp(-1-a-b&#94;Tx-x&#94;TCx) \\end{align} It is a Gaussian distribution, with constraints: \\begin{align} \\int p\\dd V&=1\\\\ \\int x_ip\\dd V&=0\\\\ \\int x_ix_jp\\dd V&=\\Sigma_{ij}\\\\ \\end{align} and we can determine $a,b,c$ from constraints. Multivariable case is harder, but the method is the same: using multiplier and EL equation. # Linear Gaussian system \\begin{align} p(x)&=N(x|\\mu_x, \\Sigma_x)\\\\ p(y|x)&=N(y|Ax+b,\\Sigma_y) \\end{align} As $y=Ax+b$, we find $x=A&#94;{-1}(y-b)$, so $$(y-Ax-b)&#94;T\\Sigma_y(y-Ax-b)=[x-A&#94;{-1}(y+b)]&#94;TA&#94;T\\Sigma_y&#94;{-1}A[x-A&#94;{-1}(y+b)]$$ Thus \\begin{align} N(y|Ax+b,\\Sigma_y)&=N\\left(x|A&#94;{-1}(y-b), A&#94;{-1}\\Sigma_yA&#94;{-T}\\right)\\\\ &=N_c(x|A&#94;T\\Sigma_y&#94;{-1}AA&#94;{-1}(y-b), A&#94;T\\Sigma_y&#94;{-1}A)\\\\ &=N_c(x|A&#94;T\\Sigma_y&#94;{-1}(y-b), A&#94;T\\Sigma_y&#94;{-1}A)\\\\ &=N_c(y|\\Sigma_y&#94;{-1}(Ax+b), \\Sigma_y&#94;{-1}) \\end{align}\\begin{align} p(x,y)&= N(x|\\mu_x, \\Sigma_x)N(y|Ax+b,\\Sigma_y)\\\\ &=N_c(x|\\Sigma_x&#94;{-1}\\mu_x, \\Sigma_x&#94;{-1})N_c(x|A&#94;T\\Sigma_y&#94;{-1}(y-b), A&#94;T\\Sigma_y&#94;{-1}A)\\\\ &=p(y)N_c(x|\\Sigma_x&#94;{-1}\\mu_x+A&#94;T\\Sigma_y&#94;{-1}(y-b), \\Sigma_x&#94;{-1}+A&#94;T\\Sigma_y&#94;{-1}A)\\\\ &=p(y)N(x|\\mu_{x|y},\\Sigma_{x|y})\\\\ &=p(y)p(x|y)\\\\ \\end{align} Where \\begin{align} \\Sigma_{x|y}&#94;{-1}&=\\Sigma_x&#94;{-1}+A&#94;T\\Sigma_y&#94;{-1}A\\\\ \\mu_{x|y}&=\\Sigma_{x|y}[\\Sigma_x&#94;{-1}\\mu_x+A&#94;T\\Sigma_y&#94;{-1}(y-b)]\\end{align} How to calculate $p(y)$? # Attempt We can rewrite $$N_c(x|A&#94;T\\Sigma_y&#94;{-1}y+(A&#94;T\\Sigma_y&#94;{-1}b+\\Sigma_x&#94;{-1}\\mu_x), \\Sigma_x&#94;{-1}+A&#94;T\\Sigma_y&#94;{-1}A)=N_c(y|Cx+d, xx)$$ From $p(x,y)=p(y)p(x|y)$, we find $\\Sigma&#94;{-1}=\\Sigma_x&#94;{-1}+A&#94;T\\Sigma_y&#94;{-1}A-\\Sigma_{x|y}&#94;{-1}$","tags":"MLAPP","url":"https://www.peijun.me/mlapp-ch4-notes.html"},{"title":"Solutions for Chapter 4. Gaussian Models","text":"$\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\newcommand{\\pfr}[2]{\\frac{\\pp #1}{\\pp #2}} % Partial derivative \\newcommand{\\pfr}[2]{\\frac{\\pp #1}{\\pp #2}} % Partial derivative \\newcommand{\\inv}[1]{#1&#94;{-1}} % Inverse Matrix \\newcommand{\\invt}[1]{#1&#94;{-T}} % Inverse Transposed Matrix \\renewcommand{\\nl}{\\\\&\\phantom{{}={}}}% Newline In aligned equations \\newcommand{\\dfr}[2]{\\frac{\\dd #1}{\\dd #2}} % Total derivative \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS}$ # 4.1 As $$\\E[x]=0,\\quad \\E[y]=\\E[x]&#94;2+\\Var[x]&#94;2=1/3$$ \\begin{align} \\Cov[x,y]&=E[xy-x\\bar y-\\bar xy+\\bar x\\bar y]\\\\ &=\\E[xy]-\\E[x]\\E[y]\\\\ &=\\int_{-1}&#94;1 \\frac{x&#94;3}{2}dx\\\\ &=0\\\\ &\\Rightarrow\\rho=0\\end{align} # 4.2 $$P(Y=y)=\\frac{P(X=y)+P(X=-y)}{2}=P(X=y)$$ It is obvious that $\\E[x]=\\E[y]=0$, \\begin{align} \\Cov[x,y]&=\\E[xy]-\\E[x]\\E[y]\\\\ &=P(W=1)\\E[x,y|W=1]+P(W=-1)\\E[x,y|W=-1]\\\\ &=(E[x&#94;2]+E[-x&#94;2])/2\\\\ &=0 \\end{align} # 4.3 We need to prove $|\\rho|<1$, i.e. $$\\Cov[x,y]&#94;2\\leq \\Cov[x,x]\\Cov[y,y]$$ The $\\Cov$ function satisty Symmetry $\\Cov[x,y]=\\Cov[y,x]$ Double linearity $\\Cov[\\lambda x+\\mu y, z]=\\lambda\\Cov[x, z]+\\mu\\Cov[y,z]$ Positive definiteness $\\Cov[x, x]\\geq 0$, equal sign holds iff $x=0$ Using Cauchy-Schwartz Inequality, the conclusion is straightforward. # 4.4 It is obvious that $\\Cov[X,Y+c]=\\Cov[X,Y]$ for any const $c$. So for $Y=aX+b$, \\begin{align} \\Cov[X,Y]&=\\Cov[x,aX]=a\\Cov[x,x]\\\\ \\Cov[Y,Y]&=a&#94;2\\Cov[X,X] \\end{align} So we have $$\\rho=\\frac{\\Cov[X,Y]}{\\sqrt{\\Cov[X,X]\\Cov[Y,Y]}}=\\frac{a\\Cov[X,X]}{|a|\\Cov[X,X]}=\\sgn[a]$$ # 4.5 Suppose $\\Lambda=\\Sigma&#94;{-1}=U&#94;TDU$, where $U$ is orthonormal and $D$ is diagonal, then we can find basis $\\vec y=U(\\vec x-\\vec{\\mu})$ , which simplifies the integral to \\begin{align} \\int \\exp\\left(-\\frac{y&#94;TDy}{2}\\right)&=\\prod \\int \\exp\\left(-\\frac{D_iy_i&#94;2}{2}\\right)dy_i\\\\ &=\\prod \\sqrt{\\frac{2\\pi}{D_i}}\\\\ &=\\frac{(2\\pi)&#94;{d/2}}{\\sqrt{\\det D}}\\\\ &=(2\\pi)&#94;{d/2}\\sqrt{\\det \\Sigma}\\\\ \\end{align} # 4.6 Obviously, $\\det\\Sigma=(1-\\rho&#94;2)\\sigma_1&#94;2\\sigma_2&#94;2$ The inverse matrix of $\\Sigma$ is \\begin{align} \\Lambda&=\\adj\\Sigma/\\det\\Sigma\\\\ &=\\begin{bmatrix} \\sigma_2&#94;2& -\\rho\\sigma_1\\sigma_2\\\\ -\\rho\\sigma_1\\sigma_2& \\sigma_1&#94;2 \\end{bmatrix}/\\left[(1-\\rho&#94;2)\\sigma_1&#94;2\\sigma_2&#94;2\\right]\\\\ &=\\frac{1}{1-\\rho&#94;2}\\begin{bmatrix} \\dfrac{1}{\\sigma_1&#94;2}& -\\dfrac{\\rho}{\\sigma_1\\sigma_2}\\\\ -\\dfrac{\\rho}{\\sigma_1\\sigma_2}& \\dfrac{1}{\\sigma_2&#94;2} \\end{bmatrix} \\end{align} Plug thess expressions into the original pdf, we can easily prove the (4.268) # 4.7 Bivariate conditioning \\begin{align} p(x_1,x_2)&=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho&#94;2}}\\exp\\left(-\\frac{1}{2(1-\\rho&#94;2)}\\left(\\frac{(x_1-\\mu_1)&#94;2}{\\sigma_1&#94;2}+\\frac{(x_2-\\mu_2)&#94;2}{\\sigma_2&#94;2}-2\\rho \\frac{x_1-\\mu_1}{\\sigma_1}\\frac{x_2-\\mu_2}{\\sigma_2}\\right)\\right)\\\\ & =\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho&#94;2}}\\exp\\left(-\\frac{1}{2(1-\\rho&#94;2)}\\left(\\frac{x_1-\\mu_1}{\\sigma_1}-\\rho\\frac{x_2-\\mu_2}{\\sigma_2}\\right)&#94;2+\\frac{(x_2-\\mu_2)&#94;2}{2\\sigma_2&#94;2}\\right)\\\\ & =\\frac{N(x_2|\\mu_2,\\sigma_2&#94;2)}{\\sqrt{2\\pi}\\sigma_1\\sqrt{1-\\rho&#94;2}}\\exp\\left(-\\frac{1}{2\\sigma_1&#94;2(1-\\rho&#94;2)}\\left(x_1-\\mu_1-\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2)\\right)&#94;2\\right)\\\\ &=N(x_2|\\mu_2,\\sigma_2&#94;2)N\\left(x_1\\Big|\\mu_1+\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2),\\sigma_1&#94;2(1-\\rho&#94;2)\\right)\\\\ &=p(x_2)N\\left(x_1\\Big|\\mu_1+\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2),\\sigma_1&#94;2(1-\\rho&#94;2)\\right)\\\\ \\Rightarrow\\quad p(x_1|x_2)&=N\\left(x_1\\Big|\\mu_1+\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2),\\sigma_1&#94;2(1-\\rho&#94;2)\\right) \\end{align} If $\\sigma_i=1$, then it is simplified to $$p(x_1|x_2)=N\\left(x_1|\\mu_1+\\rho(x_2-\\mu_2),1-\\rho&#94;2\\right)$$ # 4.8 TBD Try to use python to solve it! # 4.9 Sensor fusion with known variances in 1d \\begin{align} p(\\mu|D)&\\propto \\prod_i N(y_i&#94;{(1)}|\\mu, v_1)\\prod_i N(y_i&#94;{(2)}|\\mu, v_2)\\\\ &\\propto \\prod_i N(\\mu|y_i&#94;{(1)}, v_1)\\prod_i N(\\mu|y_i&#94;{(2)}, v_2)\\\\ &\\propto \\prod_i N_c(\\mu|v_1&#94;{-1}y_i&#94;{(1)}, v_1&#94;{-1})\\prod_i N_c(\\mu|v_2&#94;{-1}y_i&#94;{(2)}, v_2&#94;{-1})\\\\ &\\propto N_c\\left(\\mu\\Big|v_1&#94;{-1}\\sum_i y_i&#94;{(1)}+v_2&#94;{-1}\\sum_i y_i&#94;{(2)}, n_1v_1&#94;{-1}+n_2v_2&#94;{-1}\\right)\\\\ &\\propto N_c\\left(\\mu\\Big|n_1v_1&#94;{-1}\\bar y&#94;{(1)}+n_2v_2&#94;{-1}\\bar y&#94;{(2)}, n_1v_1&#94;{-1}+n_2v_2&#94;{-1}\\right)\\\\ &\\propto N\\left(\\mu\\Big|\\frac{n_1\\bar y&#94;{(1)}/v_1+n_2\\bar y&#94;{(2)}/v_1}{n_1/v_1+n_2/v_2}, \\frac{1}{n_1/v_1+n_2/v_2}\\right) \\end{align} So the mean of $\\mu$ is $\\dfrac{n_1\\bar y&#94;{(1)}/v_1+n_2\\bar y&#94;{(2)}/v_1}{n_1/v_1+n_2/v_2}$, and variance of $\\mu$ is $\\dfrac{1}{n_1/v_1+n_2/v_2}$. # 4.10 Information form marginalization formula $$\\det \\Lambda=\\det \\Sigma&#94;{-1}=(\\det\\Sigma)&#94;{-1}$$\\begin{align} (x-\\mu)&#94;T\\Sigma&#94;{-1}(x-\\mu)&=x&#94;T\\Lambda x-2x&#94;T\\Lambda \\mu+\\mu&#94;T\\Lambda\\mu\\\\ &=x&#94;T\\Lambda x-2x&#94;T\\xi+\\xi&#94;T\\Lambda&#94;{-1}\\xi \\end{align} From $$\\begin{bmatrix} \\Sigma_{11}& \\Sigma_{12}\\\\ \\Sigma_{21}& \\Sigma_{22} \\end{bmatrix} \\cdot\\begin{bmatrix} \\Lambda_{11}&\\Lambda_{12}\\\\ \\Lambda_{21}&\\Lambda_{22} \\end{bmatrix}=I,$$ we find \\begin{align} \\Sigma_{11}&#94;{-1}=\\Lambda_{11}-\\Lambda_{12}\\Lambda_{22}&#94;{-1}\\Lambda_{21},\\quad \\Sigma_{12}&#94;{-1}=\\Lambda_{21}-\\Lambda_{22}\\Lambda_{12}&#94;{-1}\\Lambda_{11}\\\\ \\Sigma_{21}&#94;{-1}=\\Lambda_{12}-\\Lambda_{11}\\Lambda_{21}&#94;{-1}\\Lambda_{22},\\quad \\Sigma_{22}&#94;{-1}=\\Lambda_{22}-\\Lambda_{21}\\Lambda_{11}&#94;{-1}\\Lambda_{12} \\end{align} Using the rule $N(\\mu,\\Sigma)\\rightarrow N_c(\\Sigma&#94;{-1}\\mu,\\Sigma&#94;{-1})$ \\begin{align} p(x_2)&=N(x_2|\\mu_2,\\Sigma_{22})\\\\ &=N_c(x_2|\\Sigma_{22}&#94;{-1}\\mu_2, \\Sigma_{22}&#94;{-1})\\\\ &=N_c(x_2|(\\Lambda_{22}-\\Lambda_{21}\\Lambda_{11}&#94;{-1}\\Lambda_{12})\\mu_2,\\Sigma_{22}&#94;{-1})\\\\ &=N_c(x_2|(\\Lambda_{22}\\mu_2+\\Lambda_{21}\\mu_1)-\\Lambda_{21}\\Lambda_{11}&#94;{-1}(\\Lambda_{11}\\mu_1+\\Lambda_{12}\\mu_2),\\Sigma_{22}&#94;{-1})\\\\ &=N_c(x_2|\\xi_2-\\Lambda_{21}\\Lambda_{11}&#94;{-1}\\xi_1,\\Lambda_{22}-\\Lambda_{21}\\Lambda_{11}&#94;{-1}\\Lambda_{12})\\\\ p(x_1|x_2)&=N(x_1|\\mu_{1|2}, \\Sigma_{1|2})\\\\ &=N(x_1|\\Lambda_{11}&#94;{-1}(\\Lambda_{11}\\mu_1-\\Lambda_{12}(x_2-\\mu_2)), \\Lambda_{11}&#94;{-1})\\\\ &=N_c(x_1|\\Lambda_{11}\\mu_1-\\Lambda_{12}(x_2-\\mu_2),\\Lambda_{11})\\\\ &=N_c(x_1|(\\Lambda_{11}\\mu_1+\\Lambda_{12}\\mu_2)-\\Lambda_{12}x_2,\\Lambda_{11})\\\\ &=N_c(x_1|\\xi_1-\\Lambda_{12}x_2,\\Lambda_{11})\\\\ \\end{align} # 11，12 # 13 $$p(\\mu|D)\\propto N(\\mu|\\mu_0,9)\\prod_i N(y_i|\\mu, 4)$$ And we find $\\mu\\sim N(\\mu_n,\\sigma_n&#94;2)$, where $\\sigma_n&#94;2=\\dfrac{1}{1/9+n/4}$, so $n=4/\\sigma_n&#94;2-4/9$. We need $$1.96\\sigma_n=1/2\\quad\\Rightarrow n>62.29$$","tags":"MLAPP","url":"https://www.peijun.me/mlapp-ch4-sol.html"},{"title":"Solutions for Chapter 3 Generative models for discrete data","text":"$\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS} \\DeclareMathOperator{\\Beta}{B}$ # 3.1 $p'/p=N_1/\\theta-N_0/(1-\\theta)=0\\quad\\Rightarrow \\theta=N_1/(N_0+N_1)$ # 3.2, 3.3 Obvious (3.90) should be $\\Gamma(x+1)=x\\Gamma(x)$ # 3.4 $\\Beta(\\theta|1,1)=\\operatorname {Unif}(0,1)$, $\\theta$ is probability for heads. $$P(\\theta|X<3)=\\frac{P(\\theta,X<3)}{P(X<3)}$$ \\begin{align} P(\\theta,X<3) &= P(X<3|\\theta)\\\\ &=\\sum_{i=0}&#94;2\\binom{5}{i}\\theta&#94;i(1-\\theta)&#94;{5-i}\\\\ &=(1+3\\theta+6\\theta&#94;2)(1-\\theta)&#94;3 \\end{align}$$P(X<3)=\\int_0&#94;1 (1+3\\theta+6\\theta&#94;2)(1-\\theta)&#94;3d\\theta=\\frac{1}{2}$$$$P(\\theta|X<3)=2(1+3\\theta+6\\theta&#94;2)(1-\\theta)&#94;3$$ # 3.5 \\begin{align} p(\\theta)&\\propto p(\\phi(\\theta))\\frac{d\\phi}{d\\theta}\\\\ &\\propto \\frac{1}{\\theta}+\\frac{1}{1-\\theta}\\\\ &\\propto \\theta&#94;{-1}(1-\\theta)&#94;{-1}\\\\ &\\propto\\Beta(\\theta|0,0) \\end{align} # 3.6 Suppose we have data set $\\mathcal{D}={x_1,x_2,\\ldots,x_n}$ Log-Likelihood is $$\\ln L=-n\\lambda+\\sum x_i\\ln\\lambda-\\sum\\ln(x_i!)$$ $$d\\ln L/d\\lambda=\\sum x_i/\\lambda-n=0\\Rightarrow \\lambda=\\bar x$$ # 3.7 # a. \\begin{align} p(\\lambda|D)&\\propto p(D|\\lambda)p(\\lambda)\\\\ &\\propto e&#94;{-(n+b)\\lambda}\\lambda&#94;{n\\bar x+a-1}\\\\ &\\propto \\operatorname{Ga}(\\lambda|n\\bar x+a,n+b) \\end{align} # b. $$\\bar\\lambda=\\frac{n\\bar x+a}{n+b}$$ It will go to $\\bar x$, which is the MLE given in 3.6 # 3.8 The MLE is $\\hat a=\\max{|x_i|}$. As $$L=\\prod_i I(a\\geq |x_i|)\\frac{1}{2a}=\\frac{I(a\\geq \\max{|x_i|})}{2&#94;na&#94;n}$$ The problem is that $\\hat a$ in at the edge of probable $a$. So it is always smaller than real value, i.e. biased. # 3.9 There is an error in exercise. Theer should be $I(\\theta>b)$ term in the pdf. Define $M=\\max\\{x_1,x_2,\\ldots,x_n,b\\}$ $$p(\\theta|D)=\\frac{p(\\theta,D)}{p(D)}=\\frac{(N+K)M&#94;{N+K}}{\\theta&#94;{N+K+1}}I(\\theta>M)$$ It is a Pareto dist start at $M$ with exponent $N+K$ # 3.15 $$m=\\frac{a}{a+b},\\quad v=\\frac{ab}{(a+b)&#94;2(a+b+1)}$$ We have $$v=\\frac{m(1-m)}{a+b+1}\\quad\\Rightarrow\\quad a+b=\\frac{m(1-m)-v}{v}$$ As a result $$a=\\left[\\frac{m(1-m)-v}{v}\\right]m, \\quad b=\\left[\\frac{m(1-m)-v}{v}\\right](1-m)$$ # 3.16 $$\\frac{a}{m}=\\frac{b}{1-m}=k$$ So the probability inside $[l,u]$ is $$p(k)=\\int_l&#94;u \\frac{x&#94;{mk-1}(1-x)&#94;{(1-m)k-1}}{B[mk, (1-m)k]}dx=0.95$$ With $l=0.05,u=0.3,m=0.15$ given, we can find root $k=30.0404$, that is $$a=4.50606, \\quad b=25.5343$$ # 3.17 $$p(N_1|N,\\theta)= \\binom{N}{N_1}\\theta&#94;{N_1}(1-\\theta)&#94;{N-N_1}$$ So we find \\begin{align} p(N_1|N)&=\\binom{N}{N_1}\\int \\theta&#94;{N_1}(1-\\theta)&#94;{N-N_1}d\\theta\\\\ &=\\binom{N}{N_1}B(N_1+1, N-N_1+1)\\\\ &=1/(N+1) \\end{align} # 3.19 # a. We have $P(c|x)=P(x|c)P(c)/P(x)$. As $P(x),P(c)=0.5$ won't affect the ratio, $$K=\\log\\frac{P(c=1|x)}{P(c=2|x)}\\propto \\log\\frac{P(x|c=1)}{P(x|c=2)}=\\mathbf{\\phi}&#94;T(\\mathbf{\\beta}_1-\\mathbf{\\beta}_2)$$ Define $$K_w=x_w\\log\\frac{\\theta_{1w}}{\\theta_{2w}}+(1-x_w)\\log\\frac{1-\\theta_{1w}}{1-\\theta_{2w}},$$ then $$K=\\sum_w K_w$$ # b. For indiscrimitive word, $K_w$ should not be changed significantly by $x_w$, so if $$\\delta_w=\\log \\frac{\\theta_{1w}(1-\\theta_{2w})}{\\theta_{2w}(1-\\theta_{1w})}=\\beta_{1w}-\\beta_{2w}$$ is small enough, we may consider $w$ irrevalent. # c. $$\\hat\\theta_{cw}=\\frac{1+n_c}{2+n_c}$$ So, $\\beta_{1w}-\\beta_{2w}$ will be heavily affected by $n_2/n_1$. It will not be ignored. The reason is that the word is strongly affected by regularization to the class. Consider the postorior distribution of $\\theta$ $$L(\\theta)=\\operatorname{Beta}(k,n-k), \\qquad k=n_{cw}+1, \\quad n=n_w+2$$ $$\\bar\\theta=k/n,\\quad \\sigma=\\frac{1}{n}\\sqrt{\\frac{k(n-k)}{(n+1)}}$$ We need $$0<\\bar\\theta\\pm A\\sigma< 1$$ $$\\Rightarrow \\sqrt k>A,\\quad \\sqrt{n-k}>A$$ i.e. $$n_{cw}>A&#94;2, \\quad n_c-n_{cw}>A&#94;2$$ # d. Now we begin to consider the variance of $K_w$ because of the keyword $w$: $$\\sigma&#94;2_w=\\delta_w\\theta_w(1-\\theta_w)$$ The variance of $K$ is $$\\sigma&#94;2=\\sum_w \\sigma_w&#94;2$$ We can select keywords with largest $\\sigma_w&#94;2=\\delta_w\\theta_w(1-\\theta_w)$, and (a.) is a special case of this criteria. # 3.20 # a. $$p(x|y=c)=p(x_1|\\theta_{jc})p(x_2|x_1,\\theta_{jc})\\cdots p(x_D|x_{D-1},\\ldots x_1,\\theta_{jc})$$ So the number of parameters we need is $$(2+2&#94;2+\\cdots + 2&#94;{D})C=(2&#94;{D+1}-2)C$$ # b. c. Naive Bayes will be better for small data set, while Full Bayes is more accurate for large data set. Because there are few parameters, so we tend to get better estimation in naive case. Conversely, the full case need much more data to train the parameters. # d. $O(ND)$ for naive $O(N2&#94;D)$ for full # e. linear $O(D)$ for naive model quadratic $O(D&#94;2)$ for full # f. What does missing data mean? # 3.21 \\begin{align} I&=\\sum_{x_j}\\sum_y p(x_j|y)p(y)\\log\\frac{p(x_j|y)}{p(x_j)}\\\\ &=\\sum_c\\pi_c\\sum_{x_j} \\theta_{jc}&#94;{x_j}(1-\\theta)_{jc}&#94;{1-x_j}\\log\\frac{\\theta_{jc}&#94;{x_j}(1-\\theta)_{jc}&#94;{1-x_j}}{\\sum_c\\pi_c \\theta_{jc}&#94;{x_j}(1-\\theta)_{jc}&#94;{1-x_j}}\\\\ &=\\sum_c \\pi_c\\left[\\theta_{jc}\\log\\frac{\\theta_{jc}}{\\sum_c \\pi_c\\theta_{jc}}+\\sum_c (1-\\theta_{jc})\\log\\frac{1-\\theta_{jc}}{\\sum_c \\pi_c(1-\\theta_{jc})}\\right]\\\\ &=\\sum_c \\pi_c\\left[\\theta_{jc}\\log\\frac{\\theta_{jc}}{\\theta_{j}}+\\sum_c (1-\\theta_{jc})\\log\\frac{1-\\theta_{jc}}{1-\\theta_{j}}\\right] \\end{align} # 3.22 In 3 spam messages, we find: secret x 2 dollar x 1 In 4 non-spam messages, we find: secret x 1 sports x 2 As a result, we can get MLEs: $\\theta_{\\mathrm{spam}}=3/7$ $\\theta_{\\mathrm{secret|spam}}=2/3$ $\\theta_{\\mathrm{secret|non-spam}}=1/4$ $\\theta_{\\mathrm{sports|non-spam}}=2/4=1/2$ $\\theta_{\\mathrm{dollar|spam}}=1/3$","tags":"MLAPP","url":"https://www.peijun.me/mlapp-ch3-sol.html"},{"title":"Notes on Chapter 2 Probability","text":"$\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS} \\DeclareMathOperator{\\Beta}{B}$ # Multinomial Integral For a group of variables $x_i>0$ with constraint $\\sum_{i=1}&#94;{n} x_i$. We define tail sum of exponents $p_i>0$ to be $S_k=\\sum_{i=k}&#94;n p_i$, and $T_k=\\sum_{i=k}&#94;{n} x_i$. Define Integral operator $$\\hat \\int_k=\\prod_{i=1}&#94;k\\int_0&#94;{1-S_{i-1}} x_i&#94;{p_i-1}dx_i$$ we can write multi-integral: \\begin{align} \\Beta(\\mathbf{p}) &=\\hat\\int_n \\delta\\left(1-S_n\\right)=\\hat \\int_{n-1}(1-S_{n-1})&#94;{T_n-1}\\\\ &=\\Beta(T_n,p_{n-1})\\hat \\int_{n-2} (1-S_{n-2})&#94;{T_{n-1}-1}\\\\ &=\\prod_{i=1}&#94;{n-1}\\Beta(T_{i+1},p_i)=\\prod_{i=1}&#94;{n-1}\\frac{\\Gamma(T_{i+1})}{\\Gamma(T_i)}\\Gamma(p_i)\\\\ &=\\frac{\\Gamma(T_n)}{\\Gamma(T_1)}\\prod_{i=1}&#94;{n-1}\\Gamma(p_i)=\\frac{\\prod_i\\Gamma(p_i)}{\\Gamma(\\sum_i p_i)} \\end{align} If we have $\\mathbf{p'}=\\mathbf{p}+(1,0,\\ldots,0)$, then $$\\Beta(\\mathbf{p'})=\\frac{p_0}{\\sum p_i}\\Beta(\\mathbf{p})$$","tags":"MLAPP","url":"https://www.peijun.me/mlapp-ch2-notes.html"},{"title":"Solutions for Chapter 2 Probability","text":"$\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} \\newcommand{\\bm}{\\mathbf} \\DeclareMathOperator{\\Var}{Var} \\DeclareMathOperator{\\det}{det} \\DeclareMathOperator{\\tr}{tr} \\DeclareMathOperator{\\E}{E} \\DeclareMathOperator{\\Cov}{Cov} \\DeclareMathOperator{\\Beta}{B} \\DeclareMathOperator{\\Bdist}{Beta} \\DeclareMathOperator{\\sgn}{sgn} \\DeclareMathOperator{\\adj}{adj} \\DeclareMathOperator{\\ii}{i} \\DeclareMathOperator{\\dd}{d} \\newcommand{\\pp}{\\partial} \\DeclareMathOperator{\\rhs}{RHS} \\DeclareMathOperator{\\lhs}{LHS} \\DeclareMathOperator{\\Beta}{B}$ # 2.1 $P=1/3$ $P=1/2$, the difference is that you happen to see a child. # 2.2 Obvious # 2.3 \\begin{align} \\Var[x+y]&=\\E[(x+y)&#94;2]-\\E[x+y]&#94;2\\\\ &=\\E[x&#94;2]+\\E[y&#94;2]+2\\E[xy]-\\E[x]&#94;2-\\E[y]&#94;2-2\\E[x]\\E[y]\\\\ &=\\Var[x]+\\Var[y]+2\\Cov[x,y] \\end{align} # 2.4 $I, N$ means infected and not infected, $P$ means positive $$\\Pr(I|P)=\\frac{\\Pr(IP)}{\\Pr(P)}=\\frac{\\Pr(IP)}{\\Pr(IP)+\\Pr(NP)}\\approx\\frac{10&#94;{-4}}{10&#94;{-4}+0.01}\\approx 0.01$$ # 2.5 The door we chose has probability $1/3$ for the prize. After the host has revealed another invalid door, the chance of the last door is $1-1/3=2/3$. Note the host is not randomly removing a door, so the chance of the two unchosen doors will fall to the last door. # 2.6 $$P(H|e_1,e_2)=\\frac{P(H,e_1,e_2)}{P(e_1,e_2}=\\frac{P(e_1,e_2|H)P(H)}{P(e_1,e_2)}$$ Only ii is sufficient Given $P(e_1,e_2|H)=P(e_1|H)P(e_2|H)$, we have $$P(H|e_1,e_2)=\\frac{P(e_1,H)P(e_2,H)}{P(H)P(e_1,e_2)}$$ Use $P(e_1,e_2)=\\sum_h P(e_1,e_2|H=h)P(H=h)$, i is equivalent to iii Both i, iii are sufficient now # 2.7 Pairwise independence means $$P(X_iX_j)=P(X_i)P(X_j), \\forall i\\neq j\\in \\{1,\\ldots ,n\\}$$ Mutually independent means $$P(\\prod_{X\\in S}X)=\\prod_{X\\in S}, \\forall S\\subseteq \\{1,\\ldots ,n\\}$$ Thus for $n\\geq 3$, the two group of identities are not necessarily equivalent. # Counter example $$X, Y\\sim \\operatorname{Ber}(0.5), \\quad Z=X\\oplus Y\\sim \\operatorname{Ber}(0.5)$$ # 2.8 We want to prove $$P(x,y|z)=P(x|z)P(y|z)\\Leftrightarrow P(x,y|z)=g(x,z)h(y,z)$$ The LHS to RHS is obvious. We are now going from RHS to LHS. First of all, $p(x,y|z)$ must satisfy unitary condition $$\\iint p(x,y|z)dxdy=\\left(\\int g(x,z)dx\\right)\\left(\\int h(y,z)dy\\right)=1$$ and $$P(x|z)= g(x,z)\\int h(y,z)dy,\\qquad P(y|z)= h(y,z)\\int h(x,z)dx$$ And the deduction to LHS is trivial now. # 2.9 $$X\\perp W|Z,Y\\Rightarrow P(XWZY)=P(XZY)P(WZY)/P(ZY)$$$$X\\perp Y|Z\\Rightarrow P(XYZ)=P(XZ)P(YZ)/P(Z)$$$$X\\perp Y|W\\Rightarrow P(XYW)=P(XW)P(YW)/P(W)$$ # a. True We need to prove $$P(XYWZ)=P(XZ)P(YWZ)/P(Z),$$ which is the result of plugging 2nd equation into 1st one. # b. False We need to prove $$P(XYZW)=P(XZW)P(YZW)/P(ZW),$$ which is not reachable. # 2.10 The pdf of $y=1/x$ is \\begin{align} g(y)&=f(x(y))\\left|\\frac{dx}{dy}\\right|=f(1/y)/y&#94;2\\\\ &=\\frac{b&#94;a}{\\Gamma(a)}(1/y)&#94;{a-1}e&#94;{-b/y}/y&#94;2\\\\ &=\\frac{b&#94;a}{\\Gamma(a)}y&#94;{-(a+1)}e&#94;{-b/y} \\end{align} # 2.11 \\begin{align} Z&#94;2&=\\int_0&#94;{2\\pi}\\int_0&#94;\\infty r\\exp\\left(-\\frac{r&#94;2}{2\\sigma&#94;2}\\right)drd\\theta\\\\ &=2\\pi\\int_0&#94;\\infty\\exp\\left(-\\frac{t}{\\sigma&#94;2}\\right)dt,\\quad t=r&#94;2/2\\\\ &=2\\pi\\sigma&#94;2 \\end{align} # 2.12 Notice $-\\sum_x\\sum_y p(x,y)\\log p(x)=-\\sum_x p(x)\\log p(x)=H(X)$ \\begin{align} I(X,Y)&=\\sum_x\\sum_y p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}\\\\ &=H(X)+H(Y)-H(X,Y) \\end{align}\\begin{align} H(Y|X)&=-\\sum_y \\log p(x,y)\\log\\frac{p(x,y)}{p(x)}\\\\ &=H(X,Y)-H(X) \\end{align} So $I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$ # 2.13 Suppose we have $$X_{\\pm}=\\frac{X_1\\pm X_2}{\\sqrt{2}}$$ then $$\\begin{bmatrix} X_+\\\\ X_- \\end{bmatrix}\\sim N\\left(\\mathbf{0},\\begin{bmatrix}\\sigma_+&#94;2&\\\\&\\sigma_-&#94;2\\end{bmatrix}\\right)=N(0,\\sigma_+&#94;2)N(0,\\sigma_-&#94;2)$$ And $$\\sigma_+&#94;2\\frac{(X_1+X_2)&#94;2}{2}+\\sigma_-&#94;2\\frac{(X_1-X_2)&#94;2}{2}=\\sigma&#94;2 (X_1&#94;2+X_2&#94;2)-2\\rho X_1X_2$$ Thus $\\sigma_\\pm=(1\\pm\\rho)\\sigma&#94;2$. And $X_+$ is independent on $X_-$. From $X_\\pm$, we can get the distribution of $X_1$ and $X_2$. If $\\rho=1$, then $X_1=X_2=X/\\sqrt 2$, the mutual information $I(X_1,X_2)=H(X_1)$ If $\\rho=-1$, similiar. If $\\rho=0$, they are independent. So $I(X_1,X_2)=0$ # 2.14 $r=1-H(Y|X)/H(X)=(H(X)-H(Y|X))/H(X)=I(Y, X)/H(x)$ It is obvious that $H(X)>0$. $r\\geq 0$ because we have $I(X,Y)\\geq 0$ using Jensen inequality: $$I(X,Y)=-\\sum_{i,j} \\Pr(i,j)\\log \\frac{p_ip_j}{\\Pr(i,j)}\\geq - \\log \\sum_{i,j}p_ip_j=0$$ We need to prove $H(Y|X)\\geq 0$. And $H(Y|X)=H(X,Y)-H(Y)\\geq 0$. \\begin{align} H(X,Y)&=-\\sum_i\\sum_j \\Pr(i,j)\\log \\Pr(i,j)\\\\ &\\geq -\\sum_i p_i\\log \\frac{\\sum_j p&#94;2(i,j)}{p_i}\\\\ &\\geq -\\sum_i p_i\\log \\frac{p_i&#94;2}{p_i}=H(Y) \\end{align} As a result, $r\\leq 1$ $r=0$ holds iff $I(X,Y)=0$. In this case, $\\dfrac{p_ip_j}{\\Pr(i,j)}$ is a constant, so $X$ is independent on $Y$ $r=0$ holds iff $I(X,Y)=H(X)$. i.e. $X=Y$ # 2.15 MLE(Maximum Likehood Estimation) is to maximize the likehood $$\\prod_i q(x=i|\\theta)&#94;{n_i}=\\exp(n_i\\log q_i)\\propto\\exp(\\sum_ip_i\\log q_i)$$ It is equivalent to minimize $-\\sum_ip_i\\log q_i\\equiv -\\sum_ip_i\\log(q_i/p_i)$ # 2.16 $f=x&#94;{a-1}(1-x)&#94;{b-1}/\\Beta(a,b)\\propto x&#94;{a-1}(1-x)&#94;{b-1}$ $$E(x)=\\int xf dx=\\Beta(a+1,b)/\\Beta(a,b)=\\frac{a}{a+b}$$ We can prove the last step as follows: \\begin{align} \\Beta(p, q+1)&=\\int_0&#94;1 x&#94;{p-1}(1-x)&#94;{q}dx\\\\ &=\\int_0&#94;1 x&#94;{p-1}(1-x)&#94;{q-1}dx-\\int_0&#94;1 x&#94;{p}(1-x)&#94;{q-1}dx\\\\ &=\\int_0&#94;1 x&#94;{p-1}(1-x)&#94;{q-1}dx-\\int_0&#94;1 x&#94;{p}(1-x)&#94;{q-1}dx\\\\ &=\\int_0&#94;1 x&#94;{p-1}(1-x)&#94;{q-1}dx-\\frac{p}{q}\\int_0&#94;1 x&#94;{p-1}(1-x)&#94;{q}dx\\\\ &=\\Beta(p,q)-\\frac{p}{q}\\Beta(p, q+1) \\end{align} As a result, $\\Beta(p, q+1)=\\dfrac{q}{p+q}\\Beta(p,q)$. We can also use the relationship betweeen Beta and Gamma function, and $\\Gamma(p+1)=p\\Gamma(p)$ $$\\Beta(p,q+1)=\\frac{\\Gamma(p)\\Gamma(q)}{\\Gamma(p+q+1)}=\\frac{q\\Gamma(p)\\Gamma(q)}{(p+q)\\Gamma(p+q)}=\\frac{q}{p+q}\\Beta(p,q)$$ $$E(x&#94;2)=\\int x&#94;2f dx=\\Beta(a+2,b)/\\Beta(a,b)=\\frac{a}{a+b}\\frac{a+1}{a+b+1}$$$$\\frac{f'}{f}=\\frac{a-1}{x}+\\frac{b-1}{1-x}=0\\Rightarrow x_m=\\frac{a-1}{a-b}$$$$Var(x)=E(x&#94;2)-E(x)&#94;2=\\frac{ab}{(a+b)&#94;2(a+b+1)}$$ # 2.17 The cdf(cumulative distribution function) of each variable is $$F_1(x)=\\Pr(X<x)=x$$ So the cdf that the smallest one is smaller than $x$ is $$F_m(x)=1-(1-F_1(x))&#94;2=x(2-x)$$ So the density function is $f_m=F_m'=2(1-x)$. The subscript $m$ means minimum","tags":"MLAPP","url":"https://www.peijun.me/mlapp-ch2-sol.html"},{"title":"Solutions for MLAPP","text":"This project includes my solutions and notes to the book Machine Learning: a Probabilistic Perspective by Kevin Patrick Murphy when I was learning this book. The solutions are written in the Jupyter Notebook format .ipynb , which supports python/markdown cells with pretty output. Solutions are written in markdown cells with Mathjax extension for equations. Read online As long as you do not need to modify it, you do not have to install Jupyter Notebook. Desktop version site of Github also supports ipynb . But unfortunately, it looks bad and does not support predefined macros, which make it only a fallback or a fast preview. nbviewer is an official online viewer for read only purpose. my blog also has html generated for this project.","tags":"MLAPP","url":"https://www.peijun.me/solutions-for-mlapp.html"},{"title":"24 Game Solver","text":"24 game is an arithmetic game with a simple rule: Given 4 numbers and use + - * / to get 24. A simple example is 1, 2, 3, 4 , and you find 1*2*3*4=24 A more difficult one is 5, 5, 5, 1 , the answer is 5*(5-(1/5))=24 , which includes fractions. 24game project provides a powerful C++ solver for the 24 game. And you can play with the PyQt5 based graphical front end. In [1]: #! /usr/bin/env python3 # 求解24点的程序 def pmtd ( x , y ): \"Plus minus times divide\" if y == 0 : x , y = y , x if x == 0 : \"0 can not be denominator\" return [ y , - y , 0 ] return [ x + y , x - y , y - x , x * y , x / y , y / x ] def contract ( l , i , j ): \"Remove i j and add a space\" return l [ 0 : i ] + l [ i + 1 : j ] + l [ j + 1 :] + [ 0 ] def arithn ( numl , n = 24 ): '''Give out an process to give the given arithmetic result such as 24''' l = len ( numl ) if l == 1 : if abs ( numl [ 0 ] - n ) < 1e-6 : return [[ n ]] else : return False for i in range ( l - 1 ): for j in range ( i + 1 , l ): lis = pmtd ( numl [ i ], numl [ j ]) sl = contract ( numl , i , j ) for k in lis : sl [ - 1 ] = k res = arithn ( sl , n ) if res : res . append ( numl ) return res return False if __name__ == \"__main__\" : print ( \"-\" * 15 + \"求解24点\" + \"-\" * 15 ) help = \"输入一串数字，空格隔开，例如: 2 3 5 6，输入q退出\" print ( help ) while True : s = input ( \">>> \" ) if s == \"q\" : break s = s . replace ( \" \" , \",\" ) s = '[' + s + ']' try : numl = eval ( s ) except : print ( help ) continue t = arithn ( numl ) if t : for i in t [:: - 1 ]: print ( i ) ---------------求解24点--------------- 输入一串数字，空格隔开，例如: 2 3 5 6，输入q退出 >>> 5 5 5 1 [5, 5, 5, 1] [5, 5, 0.2] [5, 4.8] [24] >>> q 24game project also provides more readable output.","tags":"Puzzles","url":"https://www.peijun.me/24-game-solver.html"},{"title":"One way quantum computer","text":"There are some discrepancies between the results here and the paper. But the results are basically the same. \\(\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\ii}{\\mathrm{i}} \\newcommand{\\tr}{\\mathrm{tr}}\\) Rotation Gates This part corresond to the Gates in paper 0301052v2. $$S_{ab}=\\frac{1+Z_a}{2}+\\frac{1-Z_a}{2}Z_b=U_a+D_aZ_b$$ The \\(u_i, d_i\\) are defined by \\(\\ket{\\psi_i}=u_i\\ket{0_i}+d_i\\ket{1_i}\\) . And we have \\(Z_b\\ket{+_b}=\\ket{-_b}\\) $$\\begin{aligned} \\mathcal{R}&=\\bra{\\psi_1}\\bra{\\psi_2}\\bra{\\psi_3}\\bra{\\psi_4}S_{12}S_{23}S_{34}S_{45}\\ket{+_2}\\ket{+_3}\\ket{+_4}\\ket{+_5}\\\\ &=\\left(\\ket{+_5}\\bra{\\psi_4}U_4+\\ket{-_5}\\bra{\\psi_4}D_4\\right) \\bra{\\psi_1}\\bra{\\psi_2}\\bra{\\psi_3}S_{12}S_{23}S_{34}\\ket{+_2}\\ket{+_3}\\ket{+_4}\\\\ &=\\prod_{i=4}&#94;1 \\Big(\\ket{+_{i+1}}\\bra{0_i}u_i+\\ket{-_{i+1}}\\bra{1_i}d_i\\Big)\\\\ &=\\prod_{i=4}&#94;1 {\\frac{1}{\\sqrt{2}}\\begin{bmatrix}u_i & d_i\\\\-u_i & -d_i\\end{bmatrix}}=\\prod_{i=4}&#94;1 H\\begin{bmatrix}u_i & \\\\ & d_i\\end{bmatrix}\\\\ &=\\prod_{i=4}&#94;1 H\\mathcal{Z}_{\\phi_i},\\quad \\mathcal{Z}_\\phi=\\exp(-\\ii \\phi Z/2)\\\\ &=(H\\mathcal{Z}_\\zeta H)\\mathcal{Z}_\\eta (H\\mathcal{Z}_\\xi H)\\\\ &=\\mathcal{X}_\\zeta\\mathcal{Z}_\\eta\\mathcal{X}_\\xi\\end{aligned}$$ In the basis of \\(Z_1\\rightarrow Z_5\\) . If all measurement results \\(\\psi_i\\) are positive for directions \\((0, \\xi,\\eta,\\zeta)\\) , respectively, we can verify that the rotation matrix \\(\\mathcal{R}\\) is equivalent to \\(\\exp(-\\ii \\zeta X/2)\\exp(-\\ii \\eta Z/2)\\exp(-\\ii \\xi X/2)\\) . Hadamard gate is simply a special case. CNOT Gates This part corresponds to PRL.86.5188(Page 3, upper left corner), so we are using a different \\(S\\) : $$S_{ab}=1-\\frac{(1+Z_{a})(1-Z_{b})}{2}=D_a+U_aZ_b=U_b-D_bZ_a$$ So $$S_{ab}S_{bc}=\\frac{Z_c-Z_a}{2}+\\frac{Z_c+Z_a}{2}Z_b=U_bZ_c-D_bZ_a$$ $$\\begin{aligned} \\mathcal{C}&=\\bra{\\psi_1}\\bra{\\psi_2}S_{12}S_{23}S_{24}\\ket{+_2}\\ket{+_3}\\\\ &=\\Big(\\ket{+_{3}}\\bra{1_2}d_2+\\ket{-_3}\\bra{0_2}u_2\\Big)\\bra{\\psi_1}S_{12}S_{24}\\ket{+_2}\\\\ &=\\Big(\\ket{+_{3}}\\bra{1_2}d_2+\\ket{-_3}\\bra{0_2}u_2\\Big)\\bra{\\psi_1}U_2Z_4-D_2Z_1\\ket{+_2}\\\\ &=\\Big(\\pm_2\\ket{+_{3}}\\bra{1_2}+\\ket{-_3}\\bra{0_2}\\Big)\\Big(\\ket{0_2}\\bra{\\psi_1}Z_4-\\ket{1_2}\\bra{\\psi_1}Z_1\\Big)/2\\\\ &=\\Big( \\mp_2\\ket{+_{3}}\\bra{\\mp_1}+\\ket{-_3}\\bra{\\pm_1}Z_4\\Big)/2 \\\\\\end{aligned}$$ If \\(s_1=1, s_2=1\\) , then \\(\\mathcal{C}_n=\\ket{-_{3}}\\bra{-_1}Z_4+\\ket{+_3}\\bra{+_1}I_4\\) $$\\mathcal{C}=I_4\\otimes\\begin{bmatrix} 1& 1\\\\ 1& 1 \\end{bmatrix}+Z_4\\otimes\\begin{bmatrix} 1& -1\\\\ -1& 1 \\end{bmatrix}=\\begin{bmatrix} 1& &&\\\\ & 1&&\\\\ &&&1\\\\ &&1&\\\\ \\end{bmatrix}$$ In the basis of \\(Z_4\\otimes Z_1\\rightarrow Z_4\\otimes Z_3\\)","tags":"Physics","url":"https://www.peijun.me/one-way-quantum-computer.html"},{"title":"Cluster States","text":"In quantum information and quantum computing, a cluster state is a type of highly entangled state of multiple qubits. Cluster states are generated in lattices of qubits with Ising type interactions. A cluster C is a connected subset of a d-dimensional lattice, and a cluster state is a pure state of the qubits located on C. Another way of thinking of cluster states is as a particular instance of graph states, where the underlying graph is a connected subset of a d-dimensional lattice. Cluster states are especially useful in the context of the one-way quantum computer. $\\def\\bra#1{\\mathinner{\\langle{#1}|}} \\def\\ket#1{\\mathinner{|{#1}\\rangle}} \\def\\braket#1{\\mathinner{\\langle{#1}\\rangle}} \\newcommand{\\ii}{\\mathrm{i}} \\newcommand{\\tr}{\\mathrm{tr}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}} $ # Cluster State Hamiltion If no such a site $a$, we set $Z_a=1$. $$U=\\prod_a \\left[1-\\frac{(1+Z_{a})(1-Z_{a+1})}{2}\\right]=\\prod_a U_a,\\quad U=U&#94;{-1}$$$$\\begin{aligned} V_a&\\mdef U_{a-1}U_a=\\frac{(1-Z_{a-1})+(1+Z_{a-1})Z_a}{2}\\cdot \\frac{(1+Z_{a+1})-(1-Z_{a+1})Z_a}{2}\\\\ &=Z_{a-}+Z_{a+}Z_a, \\qquad Z_{a\\pm}\\mdef\\frac{Z_{a+1}\\pm Z_{a-1}}{2}\\end{aligned}$$ For high dimension, we should mark the $U$ by a $n$D coordinate as well as an axis: $U_{a, b|c}$. For site $a, b$, the $V$ will be $$V_{a,b}=U_{a,b|0}U_{a-1,b|0}U_{a,b|1}U_{a,b-1|1}=V_{a,b|0}V_{a,b|1},\\quad V_{a,b|i}=Z_{a,b|i-}+Z_{a,b|i+}Z_a$$ $Z_{a\\pm}$ satisfy $$Z_{a+}Z_{a-}=Z_{a-}Z_{a+}=0, \\quad Z_{a+}&#94;2-Z_{a-}&#94;2=Z_{a-1}Z_{a+1}, \\quad Z_{a+}&#94;2+Z_{a-}&#94;2=1,\\quad [Z_{a\\pm}, X_a&#94;i]=0$$ $$H_0=-\\sum_a X_a$$ For the cluster state, its Hamiltonian is $$\\begin{aligned} H&=UH_0U&#94;{-1}=-\\sum_a UX_aU=-\\sum_a V_aX_aV_a\\\\ &=-\\sum_a (Z_{a-}+Z_aZ_{a-})X_a(Z_{a-}+Z_aZ_{a+})\\\\ &=-\\sum_a Z_{a-}&#94;2X_a+Z_{a+}&#94;2Z_aX_aZ_a\\\\ &=-\\sum_a (Z_{a-}&#94;2-Z_{a+}&#94;2)X_a\\\\ &=\\sum_a Z_{a-1}X_aZ_{a+1}\\end{aligned}$$ As a generalization, we can get $$H=\\sum_{c} X_c\\prod_{d\\in n(c)} Z_d$$ If we make a global $\\phi=\\pi/2$ rotation $$\\Omega=\\prod_a\\exp\\left(-\\frac{\\ii \\phi Y_a}{2}\\right),\\quad z\\rightarrow x, x\\rightarrow -z$$ then $$H'=\\Omega H\\Omega&#94;{-1}=-\\sum_a X_{a-1}Z_aX_{a+1}$$ # The Property of Density Matrix Define $$P_i{\\mdef}X_{i-1}Z_iX_{i+1},\\quad Q_i{\\mdef}X_{i-1}X_{i+1}$$ Suppose $\\ket{\\phi_i}$ and $\\ket{\\psi_j}$ are eigenstates of odd $Z_k$ and even $Z_k$, respectively. For each state pair $\\ket{\\phi_i}, \\ket{\\phi_j}$ with same eigenvalue of $\\prod_i Z_{2i+1}$, there exist a transformation consists of $Q_i$ at even sites: $$\\ket{\\phi_i}=\\prod_{a\\in S_{ij}} Q_a\\ket{\\phi_j}$$ Sites $S_{ij}$ is determined by the two wave functions. The $\\mathrm{card}(S_{ij})$ is nonzero as long as $\\ket{\\phi_i}\\neq\\ket{\\phi_j}$ $$\\ket{\\Psi}=\\sum_{i,j}c_{ij} \\ket{\\phi_i}\\ket{\\psi_j}$$$$\\prod_{a\\in S_{ij}} P_a \\ket{\\phi_i}\\ket{\\psi_k}= \\prod_{a\\in S_{ij}} Q_aZ_a\\ket{\\phi_i}\\ket{\\psi_k}=\\ket{\\phi_j}\\prod_{a\\in S_{ij}} Z_a\\ket{\\psi_k}$$$$\\rho_{\\phi}=\\tr_{\\psi}{\\ket{\\Psi}\\bra{\\Psi}}=\\sum_{i,j}\\ket{\\phi_i}\\bra{\\phi_j}\\sum_k c_{ik}c&#94;*_{jk}\\braket{\\psi_k|\\psi_k}$$$$\\begin{aligned} \\braket{\\phi_j|\\rho_{\\phi}|\\phi_i}&=\\braket{\\phi_j|\\tr{\\left[\\prod_{a\\in S_{ij}} P_a\\ket{\\Psi}\\bra{\\Psi}\\right]}|\\phi_i}\\\\ &=\\sum_k |c_{jk}|&#94;2\\braket{\\psi_k|\\prod_{a\\in S_{ij}} Z_a|\\psi_k}\\\\ &=C\\sum_k \\prod_{a\\in S_{ij}}z_a(\\psi_k)\\\\ &=\\delta_{ij}\\end{aligned}$$ The last step holds as long as there exist a even site $a\\in S_{ij}$ which has a neighbor $b\\notin S_{ij}$. The condition is equivalent to $\\phi_i\\neq\\phi_j$. In this case, there exist a conjugate state $\\psi'_k$ which is flipped in the $a$th bit and cancels out the contribution of $\\psi_k$. # Code for 1D Cluster state In [5]: tosign = lambda x : \"+\" if x > 0 else \"-\" class State : '''Class to save a quantum state''' sep_print = True def __init__ ( self , ln = 4 , sign = 1 ): ''' ln list or the length of the all 1 list sign sign ''' self . sign = sign if isinstance ( ln , int ): self . n = ln self . l = tuple ( 1 for i in range ( ln )) else : self . n = len ( ln ) self . l = tuple ( ln ) self . l2 = self . l [ 0 :: 2 ] + self . l [ 1 :: 2 ] def mutate ( self , i ): '''Apply operator P_i to get a new state''' l = list ( self . l ) l [( i - 1 ) % self . n ] *=- 1 l [( i + 1 ) % self . n ] *=- 1 return State ( l , self . sign * l [ i % self . n ]) def __str__ ( self ): '''Convert to human readable wave function''' if self . sep_print : ls = [ tosign ( i ) for i in self . l2 ] s = '' . join ( ls ) return \" %s | %s ⟩⊗| %s ⟩\" % ( tosign ( self . sign ), s [: self . n // 2 ], s [ self . n // 2 :]) else : ls = [ tosign ( i ) for i in self . l ] s = '' . join ( ls ) return \" %s | %s ⟩\" % ( tosign ( self . sign ), s ) def __repr__ ( self ): return \"State( {0} , {1} )\" . format ( self . l , self . sign ) def __hash__ ( self ): return hash ( self . l2 ) def __eq__ ( self , rhs ): return self . l2 == rhs . l2 def __lt__ ( self , rhs ): return self . l2 < rhs . l2 def cluster_state ( L ): '''Generate cluster for length L, L should be even. Behavior of odd number is not guaranteed''' stable = set () # Stable set son = set ([ State ( L )]) # Ongoing Set while son : snew = set () # New Set for s in son : for i in range ( L ): tmp = s . mutate ( i ) if ( tmp not in stable ) and ( tmp not in son ): snew . add ( tmp ) stable . update ( son ) son = snew return sorted ( stable , reverse = True ) In [7]: for i in cluster_state ( 6 ): print ( i ) +|+++⟩⊗|+++⟩ +|+++⟩⊗|+--⟩ +|+++⟩⊗|-+-⟩ +|+++⟩⊗|--+⟩ +|+--⟩⊗|+++⟩ -|+--⟩⊗|+--⟩ +|+--⟩⊗|-+-⟩ -|+--⟩⊗|--+⟩ +|-+-⟩⊗|+++⟩ -|-+-⟩⊗|+--⟩ -|-+-⟩⊗|-+-⟩ +|-+-⟩⊗|--+⟩ +|--+⟩⊗|+++⟩ +|--+⟩⊗|+--⟩ -|--+⟩⊗|-+-⟩ -|--+⟩⊗|--+⟩ # Schmidt Decomposition Assume \\begin{aligned} \\ket{\\Psi}&=\\sum C_{ij} \\ket{\\phi_i}\\ket{\\phi_j}\\\\ &=\\sum_i \\ket{\\phi_i}\\sum_j C_{ij}\\ket{\\phi_j}\\\\ &=\\sum_i \\ket{\\phi_i}\\ket{\\psi_i},\\quad \\ket{\\psi_i}=\\sum_j C_{ij}\\ket{\\phi_j}=\\braket{\\phi_i|\\Psi} \\end{aligned} We want to prove the orthogonality $\\braket{\\psi_i|\\psi_j}=\\delta_{ij}$, i.e. \\begin{aligned} \\sum_k C&#94;*_{lk}\\bra{\\phi_k} \\sum_j C_{ij}\\ket{\\phi_j}\\equiv \\rho_{R}=\\delta_{ij} \\end{aligned} It is proved at The Character of Density Matrix . So naturally, $\\ket{\\phi_i}, \\ket{\\psi_i}=\\braket{\\phi_i|\\Psi}$ forms a Schmidt decomposition of $$\\ket{\\Psi}\\propto\\sum \\ket{\\phi_i}\\ket{\\psi_i}$$","tags":"Physics","url":"https://www.peijun.me/cluster-states.html"},{"title":"Renyi entropy of the wormholes","text":"\\(\\newcommand{\\bm}{\\mathbf}\\newcommand{\\dd}{\\mathrm{d}}\\) Rényi entropy is defined as: $$\\begin{aligned} S_n(\\alpha)&:=\\frac{1}{1-\\alpha}\\log\\left({\\sum_{a_{1\\to n}}}\\sum_{\\mu=1}&#94;{N}\\left(\\frac{p_{a_1a_2\\cdots a_n}}{N}\\right)&#94;\\alpha\\right) \\\\ &=\\frac{1}{1-\\alpha}\\log\\left(\\frac{1}{\\mathcal{D}&#94;{2\\alpha(n-1)}}{{{\\sum_{a_{1\\to n}}}N_{a_1a_2\\cdots a_n}{\\prod_{i=1}&#94;n d_{a_i}&#94;\\alpha}}}\\right) \\\\ &=\\frac{1}{1-\\alpha}\\left[\\log\\left({{{\\sum_{a_{1\\to n}}}N_{a_1a_2\\cdots a_n}{\\prod_{i=1}&#94;n d_{a_i}&#94;\\alpha}}}\\right)-\\alpha(n-1)\\log \\mathcal{D}&#94;2\\right]\\\\ &=\\frac{\\alpha(n-1)\\log \\mathcal{D}&#94;2-\\log t_n(\\alpha)}{\\alpha-1} \\end{aligned}$$ We are using $$\\begin{aligned} t_n(\\alpha)&:={{{\\sum_{a_{1\\to n}}}N_{a_1a_2\\cdots a_n}{\\prod_{i=1}&#94;n d_{a_i}&#94;\\alpha}}}\\\\ &= {\\sum_{a_{1\\to n}}}{\\sum_{x_{1\\to n-1}}}N_{x_0a_1}&#94;{x_1}N_{x_1a_2}&#94;{x_2}\\cdots N_{x_{n-1}a_n}&#94;{x_n}{\\prod_{i=1}&#94;n d_{a_i}&#94;\\alpha},\\quad x_0=x_n=1\\\\ &= {\\sum_{x_{1\\to n-1}}}\\prod_{i=1}&#94;n\\left({\\sum_{a_{1\\to n}}}d_{a_i}&#94;\\alpha N_{x_{i-1}a_i}&#94;{x_i}\\right)\\end{aligned}$$ Define $$T_{bc}(\\alpha):=\\sum_ad_a&#94;\\alpha N_{ba}&#94;c$$ Then $$t_n(\\alpha)={\\sum_{x_{1\\to n-1}}}\\prod_{i=1}&#94;nT_{x_{i-1}x_i}(\\alpha)\\left[\\bm T&#94;n(\\alpha)\\right]_{x_0x_n}=\\left[\\bm T&#94;n(\\alpha)\\right]_{11} \\label{t11}$$ It's obvious that \\(T\\) is an elementwise positive matrix. Moreover, \\(T\\) is normal: $$\\begin{aligned} \\label{key} T_{ab}T_{cb}&=\\sum_x\\sum_y d_x&#94;\\alpha d_y&#94;\\alpha \\sum_b N_{xa}&#94;b N_{yc}&#94;b\\\\ &=\\sum_x\\sum_y d_x&#94;\\alpha d_y&#94;\\alpha \\sum_b N_{x\\bar b}&#94;{\\bar a} N_{y\\bar b}&#94;{\\bar c}\\\\ &=\\sum_x\\sum_y d_x&#94;\\alpha d_y&#94;\\alpha \\sum_b N_{\\bar y b}&#94;{c}N_{\\bar xb}&#94;{a}\\\\ &=T_{ba}T_{bc}\\end{aligned}$$ Thus, the eigenvectors correspond to distinct eigenvalues must be orthogonal. Assume the eigenvalues and orthonormal eigenvectors are \\(\\bm x_i\\) with eigenvalue \\(\\lambda_i(\\alpha)\\) . The eigenvector \\(\\bm x_m:=d_i\\bm e_i/\\mathcal{D}\\) has the maximum eigenvalue because its all components are positive. The eigenvalue is $$\\lambda_{\\max}(\\alpha)=\\sum_a d_a&#94;{\\alpha+1}, \\quad\\lambda_{\\max}(1)=\\mathcal{D}&#94;2$$ We decompose \\(\\bm e_1\\) into \\(\\bm x_i\\) to calculate \\([\\bm{T}&#94;n]_{11}\\) : $$\\begin{aligned} \\bm e_1&=\\sum_i c_i\\bm x_i,\\quad c_i=\\bm e_1\\cdot \\bm x_i\\\\ [\\bm{T}_\\alpha&#94;n]_{11}&=\\bm{e}_1&#94;T\\bm{T}_\\alpha&#94;n \\bm{e}_1=\\sum_i c_i&#94;2\\lambda_i&#94;n\\\\ &=c_m&#94;2\\lambda_{\\max}&#94;n\\left[1+\\sum_{i\\neq m}\\frac{c_i&#94;2}{c_m&#94;2}\\left( \\frac{\\lambda_i}{\\lambda_{\\max}}\\right) &#94;n\\right]\\\\ &=\\frac{\\lambda_{\\max}&#94;n}{\\mathcal{D}&#94;2}\\left[1+\\Theta(k&#94;n)\\right],\\quad k:=\\frac{\\lambda_{\\mathrm{sub}}}{\\lambda_{\\max}}<1\\end{aligned}$$ $$\\begin{aligned} S_n(\\alpha)&=\\frac{\\alpha(n-1)\\log \\mathcal{D}&#94;2-\\log [\\bm{T}_\\alpha&#94;n]_{11}}{\\alpha-1}\\\\ &=n\\left[\\frac{\\alpha\\log \\lambda_{\\max}(1)-\\log \\lambda_{\\max}(\\alpha)}{\\alpha-1}\\right]-\\log \\mathcal{D}&#94;2+\\Theta(k&#94;n)\\label{al}\\end{aligned}$$ In the \\(\\alpha=1\\) case, there is no other nontrivial eigenvector except \\(\\bm x_m\\) because \\(\\bm T=\\bm x_m\\bm x_m&#94;\\mathsf{T}\\) . So the \\(\\Theta(k&#94;n)\\) term in [al] reduce to zero. $$\\begin{aligned} S_{n}(1)&=n\\frac{\\dd}{\\dd\\alpha}\\Big|_{\\alpha=1}\\Big[\\alpha\\log \\lambda_{\\max}(1)-\\log \\lambda_{\\max}(\\alpha)\\Big] -\\log \\mathcal{D}&#94;2\\\\ &=n\\Big[\\log \\mathcal{D}&#94;2-\\sum_a \\frac{d_a&#94;2\\log d_a}{\\mathcal{D}&#94;2}\\Big]-\\log \\mathcal{D}&#94;2\\\\ &=(n-1)\\log \\mathcal{D}&#94;2-n\\sum_a p_a\\log d_a\\end{aligned}$$","tags":"Physics","url":"https://www.peijun.me/renyi-entropy-of-the-wormholes.html"},{"title":"Geometrical Optics in Continuum","text":"Fermat's Principle \\(\\newcommand{\\bm}{\\mathbf}\\newcommand{\\dd}{\\mathrm{d}}\\newcommand{\\pp}{\\partial}\\) Fermat's Principle is $$\\delta s=\\delta\\int n\\dd \\ell=0$$ If we choose the path connecting two points to be $$\\bm r=\\bm r(t),\\quad t_0\\leq t\\leq t_1$$ We can write $$s=\\int n\\frac{\\dd \\ell}{\\dd t}\\dd t, \\quad \\frac{\\dd \\ell}{\\dd t}=|\\dot{\\bm r}|$$ The Lagrangian about parameter \\(t\\) is $$L=n\\frac{\\dd \\ell}{\\dd t}=n(\\bm r)|\\bm{\\dot r}|$$ then the Euler-Lagrange Equation is $$\\begin{aligned} &\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp\\bm{\\dot r}}-\\frac{\\pp}{\\pp\\bm{r}}\\right)L=0\\\\ \\bm p&=\\frac{\\pp L}{\\pp \\bm{\\dot r}}=n \\hat\\tau,\\quad\\hat{\\tau}=\\frac{\\pp |\\dot{\\bm r}|}{\\pp \\bm{\\dot r}}=\\bm{\\dot r}/|\\bm{\\dot r}|=\\frac{\\dd\\bm r}{\\dd\\ell}\\\\ \\frac{\\dd \\bm p}{\\dd t}&=\\frac{\\pp L}{\\pp \\bm{r}}=\\frac{\\dd\\ell}{\\dd t}\\nabla n\\\\ \\frac{\\dd \\bm p}{\\dd \\ell}&=\\nabla n=\\bm F\\end{aligned}$$ Generalized Momentum \\(\\bm p\\) is independent of the choice of \\(t\\) , and Examples Rectangular Coordinates $$n(x,y,z)=f(x,y),\\quad\\bm F=\\frac{\\pp f}{\\pp x}\\hat x+\\frac{\\pp f}{\\pp y}\\hat y$$ It is obvious that \\(p_z\\) are conserved. Central Gradient Define \\(\\bm L=\\bm r\\times \\bm p\\) , $$\\begin{aligned} \\frac{\\dd \\bm L}{\\dd \\ell}&=\\frac{\\dd\\bm r}{\\dd \\ell}\\times\\bm p+\\bm r\\times\\frac{\\dd\\bm p}{\\dd \\ell} \\\\&=\\hat{\\tau}\\times \\bm p+\\bm r\\times\\bm F \\\\&=\\bm r\\times\\bm F\\end{aligned}$$ \\(\\bm L\\) is conserved for a spherical symmetric distribution $$n(\\bm r)=f(r),\\quad \\bm r\\times\\bm F=\\bm 0$$ \\(L_z\\) is conserved for $$n(\\bm r)=f(r,\\theta),\\quad (\\bm r\\times\\bm F)_z=0$$","tags":"Physics","url":"https://www.peijun.me/geometrical-optics-in-continuum.html"},{"title":"Javascript 学习小记","text":"研究了一下Linux平台做笔记的软件，发现为知笔记半死不活，bug很多。其他的笔记程序尝试了leanote等各种软件都感觉难以使用，要么功能太弱，要么没有linux客户端。总计一下我想要的笔记软件的要求： 支持Markdown 支持嵌入代码块的语法高亮 支持latex输入一些基本的数学公式 电脑上使用起来方便 方便用手机查看 方便分享给其他人 软件稳定，无明显BUG，体积小，依赖少 最终我发现了 NBViewer + Jupyter Notebook + Github / Gist 的组合。 注：现在更倾向于用Jupyter Notebook+Pelican的组合了，参见 Blogging with Jupyter and Pelican 存在的问题 ：刷新麻烦，需要手动加 ?flush=true 解决方案 ：UserScript脚本 Refresh NBViewer Button 。具体请看描述。 为了写这个脚本，特意学习了一点js # 基本知识 获取元素的方法 demo=document.getElementById('demo') 还有 getElementsByTagName, getElementsByClassName 等 元素的内容 innerHTML 与属性 src , href , style 等 style.dislay='none' 可以隐藏元素， block 可以显示隐藏的 <p id=\"demo\" style=\"display:none\">Hello JavaScript!</p> 键值对的方式设置 style 的子属性 type 属性有什么用 To use an external script, put the name of the script file in the src attribute of a <script> tag # 输出 HTML可以用 document.write() 直接全部删改，或者修改元素的 innerHTML 修改文本 window.alert() 或者直接 alert() console.log() ，按F12出现控制台，用于调试 # 语法 类似于C语言，分号结尾语句（多个语句可以写在一行），等号初始化变量，注释用 // 或 /**/ 变量类型只有 var ?动态类型？ 函数用 function 前缀，不需要返回类型。函数体用花括号。 字符串用单双引号都可以，类似于Python，js可以字符串直接相加 首字母小写的驼峰命名 3 equal signs mean \"equality without type coercion\"，双等号只判断转换后 2 + 3 + \"5\" + 7 + 9 等于 '5579' ++ -- *= /= += -= typeof instanceof 5/2==.5 ** Exponentiation var cars = [\"Saab\", \"Volvo\", \"BMW\"]; 类似于py的list var x = {firstName:\"John\", lastName:\"Doe\"}; dict In HTML, all global variables will become window variables. # 循环 for ( var i of l ){ ... }","tags":"Computer","url":"https://www.peijun.me/learning-js.html"},{"title":"Jupyter Notebook的配置","text":"# 默认目录 jupyter notebook --generate-config vim /home/zpj/.jupyter/jupyter_notebook_config.py#查找dir设定notebook根目录 找到 c.NotebookApp.notebook_dir 条目，修改为启动jupyter后打开的目录，如 /home/zpj/code # 配置pylab, matplotlib默认载入 导入库，内联svg输出，需要运行： % pylab % matplotlib inline % config InlineBackend . figure_format = 'svg' # 配置ipython # 目的 设置pylab环境自动载入 为notebook设置 inline 显示，使得图片嵌入在文档中 设置矢量 svg 格式可以使生成的图像在任何屏幕下获得清晰的显示 # 步骤 创建配置文件 ipython profile create 编辑 ipython_config.py vim ~/.ipython/profile_default/ipython_config.py 修改内容为 c . InteractiveShellApp . pylab = \"inline\" c . InlineBackend . figure_formats = [ 'svg' ] c . InteractiveShellApp . pylab_import_all = True # matplotlib中文 matplotlib似乎只能使用 .ttf 格式的字体 $ fc-list :lang = zh | grep ttf#产生可用中文字体列表 /usr/share/fonts/truetype/DroidSansFallbackFull.ttf: Droid Sans Fallback:style = Regular 这里设置为Droid Sans Fallback这种常见的字体。用root权限运行以下脚本： file = \"/usr/lib64/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\" if [ -f $file ] then sed -i \"s/#*\\(font.sans-serif *:\\).*/\\1 Droid Sans Fallback/g\" $file sed -i \"s/#*\\(axes.unicode_minus *:\\).*/\\1 False/g\" $file echo \"matplotlibrc found and font is configured\" else echo \" $file not found!\" echo \"Please run locate matplotlibrc to find similiar file locattion\" fi 再重新载入matplotlib即可 In [1]: plot ( arange ( 10 ), 'o-' ) title ( '吃饭睡觉打豆豆' ); # 类似matplotlib.text.Text at xxx的处理 这是由于 plot 等画图函数的返回值导致的，加分号即可吞掉返回值 # 本地LM Roman字体 参见 http://stackoverflow.com/a/42446768 命令： FROM = /usr/share/javascript/mathjax NB = /usr/lib/python3.6/site-packages/notebook TO = $NB /static/components/MathJax JS = $NB /static/notebook/js/main.min.js DIR1 = fonts/HTML-CSS/TeX DIR2 = jax/output/HTML-CSS/fonts/TeX ln -s $FROM / $DIR1 $TO / $DIR1 ln -s $FROM / $DIR2 $TO / $DIR2 sed -i 's/\\(availableFonts:\\).*/\\1 \\[\"STIX-Web\", \"TeX\"\\],/g' $JS sed -i 's/\\(preferredFonts:\\).*/\\1 \"TeX\",/g' $JS sed -i 's/\\(webFont:\\).*/\\1 \"TeX\",/g' $JS 最后几句通过sed设置了： availableFonts: [\"STIX-Web\",\"TeX\"], preferredFont: \"TeX\", webFont: \"TeX\", 设置好后刷新即可看到新的字体：$$e&#94;x=\\sum_{n=0}&#94;\\infty \\dfrac{x&#94;n}{n!}$$ # Userstyle for jupyter Install stylish, and then install Jupyter Serif Font & Code Block Background style.","tags":"Computer","url":"https://www.peijun.me/jupyter-conf.html"},{"title":"最大子序列和系列问题","text":"In [2]: def maxsum ( l ): '''最大子序列和问题''' m = s = 0 for i in l : s = max ( s , 0 ) s += i m = max ( m , s ) return m In [3]: def maxprod ( l ): '''最大子序列积问题''' m = pos = 0 neg = 0 for i in l : pos = max ( 1 , pos ) * i neg = neg * i if i < 0 : pos , neg = neg , pos m = max ( m , pos ) return m In [45]: def minabssum ( l , pos = False ): '''pos True 最小正子序列 False 最小绝对值子序列问题''' cum = cumsum ( l ) s = [[ cum [ i ], i + 1 ] for i in range ( len ( l ))] s . append ([ 0 , 0 ]) t = sorted ( s ) print ( t ) minabs = inf for i in range ( len ( l )): if (( not pos ) or t [ i + 1 ][ 1 ] > t [ i ][ 1 ]): minabs = min ( minabs , t [ i + 1 ][ 0 ] - t [ i ][ 0 ]) return minabs","tags":"Puzzles","url":"https://www.peijun.me/max-sum-of-continuous-subsequence.html"},{"title":"Invariance of Euler-Lagrange Equations","text":"\\(\\newcommand{\\bm}{\\mathbf} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\pp}{\\partial}\\) E-L is deduced from the Hamilton's principle $$\\delta S=\\delta\\int L(\\bm q, \\dot{\\bm q}, t)dt=0$$ It is easy to find that, for \\(L'=L+\\dd f(\\bm q, t)/\\dd t\\) or change of variables \\(\\bm q\\to\\bm Q\\) , the min of \\(\\delta S\\) will not change. Here we want to prove it the hard way—using E-L equations. The original E-L Equations are: $$\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot q_i}-\\frac{\\pp}{\\pp q_i}\\right)L=0$$ Commutator \\(\\left [\\dfrac{\\mathrm{d}}{\\mathrm{d} t}, \\dfrac{\\partial}{\\partial q_i}\\right ]f(\\mathbf q, t)=0\\) Proof $$\\left[\\frac{\\dd}{\\dd t},\\frac{\\pp}{\\pp q_i}\\right]f(\\bm q, t) = \\left[\\dot q_i\\frac{\\pp}{\\pp q_i}+\\frac{\\pp}{\\pp t},\\frac{\\pp}{\\pp q_i}\\right]f(\\bm q, t)=0$$ Condition \\(L'=L+\\dfrac{\\mathrm{d} f(\\mathbf q, t)}{\\mathrm{d} t}\\) $$\\begin{aligned} \\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot q_i}-\\frac{\\pp}{\\pp q_i}\\right)L' &=\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot q_i}-\\frac{\\pp}{\\pp q_i}\\right)\\frac{\\dd f}{\\dd t}\\\\ &=\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp q_i}f-\\frac{\\pp}{\\pp q_i}\\frac{\\dd}{\\dd t}f\\\\ &=\\left[\\frac{\\dd}{\\dd t},\\frac{\\pp}{\\pp q_i}\\right]f\\\\ &=0\\end{aligned}$$ Condition \\(\\mathbf q\\to\\mathbf Q\\) We we change generalized coordinates \\(\\bm q\\to\\bm Q\\) , the Lagrangian: $$L(\\bm q, \\dot{\\bm q}, t)\\to L'(\\bm Q, \\dot{\\bm Q}, t)=L\\big[\\bm q(\\bm Q, t), \\dot{\\bm q}(\\bm Q, \\dot{\\bm Q}, t), t\\big]$$ We want to prove: $$\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot Q_i}-\\frac{\\pp}{\\pp Q_i}\\right)L'=0$$ $$\\begin{aligned} \\mathrm{LHS} &=\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp \\dot q_j}{\\pp \\dot Q_i}\\frac{\\pp}{\\pp \\dot q_j}-\\frac{\\pp q_j}{\\pp Q_i}\\frac{\\pp }{\\pp q_j}-\\frac{\\pp \\dot q_j}{\\pp Q_i}\\frac{\\pp }{\\pp\\dot q_j}\\right)L\\\\ &=\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp q_j}{\\pp Q_i}\\frac{\\pp}{\\pp\\dot q_j}-\\frac{\\pp q_j}{\\pp Q_i}\\frac{\\pp }{\\pp q_j}-\\frac{\\pp \\dot q_j}{\\pp Q_i}\\frac{\\pp }{\\pp\\dot q_j}\\right)L\\\\ &=\\left[\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp Q_i}q_j\\right)\\frac{\\pp}{\\pp \\dot q_j}+\\frac{\\pp q_j}{\\pp Q_i}\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot q_j}-\\frac{\\pp q_j}{\\pp Q_i}\\frac{\\pp }{\\pp q_j}-\\left(\\frac{\\pp}{\\pp Q_i}\\frac{\\dd}{\\dd t}q_j\\right)\\frac{\\pp }{\\pp\\dot q_j}\\right]L\\\\ &=\\left(\\left[\\frac{\\dd}{\\dd t}, \\frac{\\pp}{\\pp Q_i}\\right]q_j\\right)\\frac{\\pp}{\\pp \\dot q_j}L+\\frac{\\pp q_j}{\\pp Q_i}\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp \\dot q_j}-\\frac{\\pp }{\\pp q_j}\\right)L\\\\ &=0\\end{aligned}$$","tags":"Physics","url":"https://www.peijun.me/invariance-of-euler-lagrange-equations.html"},{"title":"Jacobi Identity for Classical Possion Bracket","text":"\\(\\newcommand{\\pp}{\\partial} \\newcommand{\\sgn}{\\mathrm{sgn}} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}}\\) Definition of Classical Possion Bracket: $$[f, g]=\\sum_{i=1, 2,\\ldots, n}\\frac{\\pp(f, g)}{\\pp(p_i, q_i)}=\\sum_{i=1, 2,\\ldots, n}\\left(\\frac{\\pp f}{\\pp p_i}\\frac{\\pp g}{\\pp q_i}-\\frac{\\pp f}{\\pp q_i}\\frac{\\pp g}{\\pp p_i}\\right)$$ If we define \\(p_{-i}\\mdef q_{i}\\) , and \\(I=\\{\\pm 1, \\pm, 2,\\ldots, \\pm n\\}\\) , we can rewrite it: $$[f, g]=\\sum_{i\\in I} \\sgn(i) \\frac{\\pp f}{\\pp p_i}\\frac{\\pp g}{\\pp p_{-i}}$$ Jacobi Identity is \\([f,[g,h]]+[g,[h,f]]+[h,[f,g]]=0\\) . With the new notation, we can rewrite it as $$\\begin{aligned} \\sum_{\\overrightarrow{fgh}}[f,[g,h]] &=\\sum_{\\overrightarrow{fgh}}\\sum_{i\\in I}\\sgn(i)\\frac{\\pp f}{\\pp p_i}\\frac{\\pp}{\\pp p_{-i}}\\left(\\sum_{j\\in I}\\sgn(j)\\frac{\\pp g}{\\pp p_j}\\frac{\\pp h}{\\pp p_{-j}}\\right)\\\\ &=\\sum_{\\overrightarrow{fgh}}\\sum_{i, j\\in I}\\sgn(ij)\\left(\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{-j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{j}}} +\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp g}{{\\pp p_{j}}}\\frac{\\pp&#94;2h}{{\\pp p_{-i}}{\\pp p_{-j}}}\\right)\\\\ &=\\sum_{\\overrightarrow{fgh}}\\sum_{i, j\\in I}\\sgn(ij)\\left(\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{-j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{j}}} +\\frac{\\pp h}{{\\pp p_{i}}}\\frac{\\pp f}{{\\pp p_{j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{-j}}}\\right)\\\\ &=\\sum_{\\overrightarrow{fgh}}\\sum_{i, j\\in I}\\sgn(ij)\\left(\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{-j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{j}}} +\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{-j}}}\\right)\\\\ &=\\sum_{\\overrightarrow{fgh}}\\sum_{i, j\\in I}\\sgn(ij)\\left(\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{-j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{j}}} -\\frac{\\pp f}{{\\pp p_{i}}}\\frac{\\pp h}{{\\pp p_{-j}}}\\frac{\\pp&#94;2g}{{\\pp p_{-i}}{\\pp p_{j}}}\\right)\\\\ &=0\\end{aligned}$$","tags":"Physics","url":"https://www.peijun.me/jacobi-identity.html"},{"title":"From d'Alembert to Lagrange","text":"Nowtonian \\(\\newcommand{\\bm}{\\mathbf} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\pp}{\\partial} \\newcommand{\\mdef}{\\overset{\\mathrm{def}}{=}}\\) Assume \\(\\bm F\\) is active force and \\(\\bm R\\) is constraint force. Then for \\(i\\) th body we have Newton's Second Law: $$\\bm F&#94;\\mathrm{tot}_i=\\bm F_i+\\bm R_i=m\\bm a_i$$ With a virtual displacement \\(\\delta \\bm x_i\\) , we define: $$\\delta W_i=\\bm F&#94;\\mathrm{tot}_i\\cdot\\delta\\bm x_i,\\quad \\delta K_i=m\\bm a_i\\cdot\\delta\\bm x_i$$ Then we have \\(\\delta W_i=\\delta K_i\\) . Define $$\\delta W=\\sum_i\\delta W_i,\\quad\\delta K=\\sum_i\\delta K_i$$ and obviously $$\\delta W=\\delta K$$ As the constraints will not do work, we can rewrite $$\\delta W=\\sum_i\\bm F_i\\cdot\\delta\\bm x_i$$ Lagrangian Statics \\(\\delta K=0\\) Principle of Virtual Work: $$\\frac{\\delta W}{\\delta q}=0$$ i.e. Minimal Potential Energy $$Q\\mdef-\\frac{\\delta V}{\\delta q}=0$$ Dynamics d'Alembert's Principle—Counterpart of Principle of Virtual Work: $$\\frac{\\delta W}{\\delta q}=\\frac{\\delta K}{\\delta q}$$ i.e. $$\\begin{aligned} Q\\mdef\\bm F_i\\cdot\\frac{\\pp \\bm x_i}{\\pp q}&=m_i\\bm a_i\\cdot\\frac{\\pp \\bm x_i}{\\pp q}\\\\ &=m_i\\frac{\\dd \\bm v_i}{\\dd t}\\cdot\\frac{\\pp \\bm x_i}{\\pp q}\\\\ &=m_i\\frac{\\dd}{\\dd t}\\left( \\bm v\\cdot\\frac{\\pp \\bm x_i}{\\pp q}\\right)-m_i\\bm v\\cdot\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp q}\\bm x_i\\\\ &=m_i\\frac{\\dd}{\\dd t}\\left(\\bm v\\cdot\\frac{\\pp \\dot{\\bm x_i}}{\\pp\\dot q}\\right)-m_i\\bm v\\cdot\\frac{\\pp}{\\pp q}\\frac{\\dd}{\\dd t}\\bm x_i\\\\ &=\\frac{\\dd}{\\dd t}\\left(m_i\\bm v_i\\cdot\\frac{\\pp\\bm v_i}{\\pp\\dot q}\\right)-m_i\\bm v_i\\cdot\\frac{\\pp \\bm v_i}{\\pp q}\\\\ &=\\frac{\\dd}{\\dd t}\\frac{\\pp T}{\\pp\\dot q}-\\frac{\\pp T}{\\pp q}\\\\ &=\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp\\dot q}-\\frac{\\pp}{\\pp q}\\right)T\\label{T}\\end{aligned}$$ For conservative or monogenic system, $$Q=\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp\\dot q}-\\frac{\\pp}{\\pp q}\\right)V\\label{V}$$ As a result of last two equations, if we define \\(L=T-V\\) , we have $$\\left(\\frac{\\dd}{\\dd t}\\frac{\\pp}{\\pp\\dot q}-\\frac{\\pp}{\\pp q}\\right)L=0$$","tags":"Physics","url":"https://www.peijun.me/from-dalembert-to-lagrange.html"},{"title":"Tn格点上逾渗的几何结构","text":"《$T_n$格点上逾渗的几何结构》是我的本科毕业论文。 论文 下载地址 幻灯片 下载地址 ，待上传 具体算法原理等请参考论文。以及章彦博写的 算法原理","tags":"Physics","url":"https://www.peijun.me/percolation-tn-lattice.html"},{"title":"LaTeX环境搭建","text":"# 安装 sudo zypper in texstudio texlive-ctex texlive-xecjk # texstudio -> kile is ok # NotoSans字体的配置 似乎过时了 \\setCJKmainfont [BoldFont=Noto Sans CJK SC Medium,ItalicFont=KaiTi] { SimSun } \\setCJKsansfont [BoldFont=Noto Sans CJK SC Medium] { Noto Sans CJK SC Regular } \\setCJKmonofont { Noto Sans CJK SC Light } \\setCJKfamilyfont { zhkai }{ KaiTi } \\setCJKfamilyfont { ltnoto }{ Noto Sans CJK SC Light } \\setCJKfamilyfont { zhsong }{ SimSun } \\setCJKfamilyfont { noto } [BoldFont=Noto Sans CJK SC Medium] { Noto Sans CJK SC Regular } \\NewDocumentCommand\\songti {}{ \\CJKfamily { zhsong }} \\NewDocumentCommand\\fangsong {}{ \\CJKfamily { zhfs }} \\NewDocumentCommand\\kaishu {}{ \\CJKfamily { zhkai }} \\NewDocumentCommand\\noto {}{ \\CJKfamily { noto }} # Kile和Okular的正反向搜索 # 正向搜索——配置Kile 设置 → 配置Kile → 工具 → 编译 →选中相应编译选项如 xelatex →右侧下拉框选中 PDF Modern →下侧参数框内添加 -synctex=1 设置 → 配置Kile → 工具 → 编译 →选中 ViewPDF →下侧参数框内添加 –unique 设置 → 配置快捷键 → 搜索栏 中搜索 ForwardPDF →修改好它的快捷键为你想用的快捷键 在配置工具栏中加一个 ForwardPDF按钮 即可。但是这样会使得失去了设置中的一些按钮。 毫无疑问这是Kile的一个Bug ，为此你可以添加一个配置Kile的按钮和配置快捷键的按钮到工具栏。 设置好了之后你就可以通过快捷键或者工具栏按钮进行正向搜索了 # 反向搜索——配置Okular 设置 → 配置Okular → 编辑器 →选择 Kile Okular处于 浏览模式 时 Shift+左键 点击，即可跳到Kile中。可能Kile的配色中当前行不够突出，你可以修改当前行的背景颜色，使得其较为 突出 。","tags":"Computer","url":"https://www.peijun.me/latex-configurtion.html"},{"title":"Linux的配置","text":"# 配置文件 .bashrc .vimrc .gitignore_global # 常用软件 # 影音多媒体 画图：矢量图 inkscape ，点阵图 gimp ，手绘 krita 影音播放： VLC ，深度播放器，网易云音乐 flash播放器：npapi火狐，ppapi谷歌 # 实用工具 分区 Gparted 虚拟机 VirtualBox 50G免费空间的网盘 megasync 密码管理器KeePassX 清理 bleachbit Dolphin文件管理器预览插件 kffmpegthumbnailer 视频预览 kdegraphics-thumbnailers pdf, svg等文档图片预览 多线程下载 KGet # 编程/代码相关 Git代码管理 git QtCreator 文本编辑器 kate Python Jupyter-notebook python3-jupyter_notebook 画图 python3-matplotlib 科学计算库 python3-scipy python3-numpy python3-sympy 等编程 数学软件 Mathematica # 阅读/笔记/文档 词典 goldendict ， 注意版本需要大于1.5才能用 .mdx 词典 词典库，可以上 PDA wiki ，或者 MEGA网盘 pdf阅读器 okular 及其依赖的数据文件 poppler-data LaTeX参见 TeXLive最小安装 ，对应的IDE有Kile/TexStudio等 pdf目录编辑器 HandyOutliner dpsprep djvu转pdf且保留目录，以及避免体积过度膨胀 手绘笔记软件 Xournal pdf编辑器/阅读器： master pdf editor ，Foxit WPS # 本地化 中文 kde-l10n-zh_CN translation-update-zh_CN bundle-lang-kde-zh_CN 字体 文泉驿微米黑 wqy-mocrohei-fonts 谷歌黑体 \"noto-sans-sc*\" 谷歌宋体 \"noto-serif-sc*\" ShadowSocks翻墙 图像界面 shadowsocks-qt 命令行python库工具 sslocal # 字体 各种闭源字体的 rpm 包， MEGA网盘 安装好wps后自带方正字体：宋黑仿楷+隶书/魏碑/姚体/行楷等 symbol-fonts WPS 依赖的数学符号等字体 archlinuxcn/ttf-wps-fonts kingsoft-phonetic-font GoldenDict 等显示音标需要的中文字体 # 常见问题 GoldenDict被程序阻挡？程序聚焦问题需要对应用窗口右键后设置，或者设定KDE全局不阻止抢占焦点 MEGA设置好同步的目录等 dns错误？ sudo vim /etc/resolv.conf ，添加 nameserver xxxx Dolphin设置好pdf等的预览功能， use common setting for all folders ，设置好 maxline ，设置好菜单栏，侧栏快捷方式 设置好打印机 设置好zsh 设置Spectacle截屏？ 防zip乱码 sudo zypper in unzip-rcc matplotlib后端设置 自定义模板后用刷新缓存用 mktexlsr pdf强制嵌入字体 gs -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sOutputFile=out.pdf in.pdf Firefox about:config 中的设置 全局缩放：进 layout.css.devPixelPerPx 允许第三方网站安装扩展 xpinstall.signature.required WebExtension userChrome.css 隐藏标签栏以及侧栏头 第一行模板 @namespace url(\"http://www.mozilla.org/keymaster/gatekeeper/there.is.only.xul\");","tags":"Computer","url":"https://www.peijun.me/linux-configuration.html"},{"title":"Use C++ Template Programming to find int of least byte width","text":"Also see gist #include <iostream> #include <cstdint> template < int N > struct least { static_assert ( N < 9 && N > 1 , \"Length beyond range 1<N<9!\" ); using int_ = typename least < N + 1 >:: int_ ; }; template <> struct least < 1 > { using int_ = int8_t ;}; template <> struct least < 2 > { using int_ = int16_t ;}; template <> struct least < 4 > { using int_ = int32_t ;}; template <> struct least < 8 > { using int_ = int64_t ;}; int main (){ least < 5 >:: int_ i5 = 666 ; printf ( \"Size of i5 is %d \\n \" , sizeof ( i5 )); }","tags":"Computer","url":"https://www.peijun.me/use-c-template-programming-to-find-int-of-least-byte-width.html"},{"title":"QtCreator使用小结","text":"# Debug 首先要看使用的是什么构建系统： qmake 不需要额外的设置 cmake Debug需要在生成配置的时候进行额外参数设置 qbs ??? QtC在左下角的Debug按钮一定要在Debug模式下运行才能有效。 进入Debug模式如果是 qmake 构建系统不需要额外配置。但是如果是 cmake 则每次都要重新生成配置。 参考 Debug with cmake and qtcreator 可知生成配置的时候应当添加参数 -DCMAKE_BUILD_TYPE=Debug # C++11 CMake: set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\") qmake: QMAKE_CXXFLAGS += -std=c++0x","tags":"Computer","url":"https://www.peijun.me/use-qt-creator-as-cpp-ide.html"},{"title":"Python plotting with infinite incontinuous point","text":"Reference: http://stackoverflow.com/questions/10377593/how-to-drop-connecting-lines-where-the-function-is-discontinuous ) In [2]: def tannan ( x , bound = 6 ): t = tan ( x ) if abs ( t ) > bound : return nan else : return t tann = vectorize ( tannan ) x = arange ( 0 , 9 * pi / 2 , pi / 100 ) plot ( x , tann ( x ));","tags":"Computer","url":"https://www.peijun.me/python-plotting-with-infinite-incontinuous-point.html"},{"title":"二维胶粒设计","text":"In [1]: from deplete.segm import * from deplete.colloids import * from deplete.curve import Curve # Triangular colloids In [2]: triangloid ( 1.4 ) . draw ( label = \"Circle\" ) triangloid ( 1.3 , 1 ) . draw ( label = \"Arcs\" ) triangloid ( 1 , 1 , 0.1 , 0.2 ) . draw ( label = \"Convex\" ) triangloid ( 0.8 , 1 , 0.1 , - 0.3 ) . draw ( label = \"Concave\" ) legend (); axis ( 'equal' ); # Basics axis('equal'); will give a 1:1 scaling .draw() will draw the curve, with matplotlib arguments .offset() will offset/etch a colloid by a positive/negative offset argument .shift() will move a curve .rotate() will rotate curve about the origin .area() will compute area of colloid In [3]: c = triangloid ( 1 , 1 , 0.2 , - 0.3 ) c . draw ( label = 'Colloid' ) c . shift ([ 2 , - 1 ]) . draw ( label = 'Shifted' ) c . rotate ( pi / 3 ) . draw ( label = 'Rotated' ) c . offset ( 0.1 ) . draw ( '--' , label = 'Offset' ) c . offset ( - 0.1 ) . draw ( '--' , label = 'Etched' ) legend (); axis ( 'equal' ); print ( \"The area of colloid is {} \" . format ( c . area ())) The area of colloid is 2.483660510623751 # Interaction between colloids The depletion potential is determined by intersection of offset boundaries show_colloids() will visualize the interaction between colloids In [4]: c1 = triangloid ( 1 , 0.5 , 0.2 , 0.4 ) . rotate ( pi / 6 ) c2 = triangloid ( 1 , 0.5 , 0.2 , - 0.4 ) . rotate ( - pi / 6 ) . shift ([ 2.2 , 0 ]) show_colloids ( c1 , c2 , 0.15 , inter = True ) # Dart-shaped colloids In [5]: dartoid ( 0 , 0.2 ) . draw ( label = \"Round\" ) dartoid ( 1 , 0.2 ) . shift ([ 2 , - 2 ]) . draw ( label = \"Sharp\" ) dartoid ( 1 , 0.4 ) . shift ([ 0 , - 2.1 ]) . draw ( label = \"Deep\" ) #dartoid(1, 1).draw(label=\"Convex\") #dartoid(0.8, 1).draw(label=\"Concave\") legend (); axis ( 'equal' ); In [6]: c1 = dartoid ( 0 , 2 - sqrt ( 3 )) . rotate ( pi / 6 ) c2 = c1 . shift ([ 2 , 0.07 ]) show_colloids ( c1 , c2 , 0.15 , inter = True ) In [7]: binding_energy ( c1 , c2 , 0.15 ) Out[7]: 0.14404994841823626","tags":"Physics","url":"https://www.peijun.me/two-dimension-colloids-design.html"},{"title":"二维胶粒的耗尽力","text":"# 基本数据结构 曲线由分段圆弧组成 直线可以看作是一种特殊的半径为无穷大的圆弧 # 优良性质 引理 分段圆弧扩展之后的曲线依然是分段圆弧 直线扩展后还是直线 半径为$R$的圆扩展$d$后是半径为$R+d$的圆 转接点扩展$d$后产生的新的曲线是圆$d$ 综上，无论怎么扩展，扩展前后曲线类型不变 In [2]: from deplete.colloids import * L , C = Line ([ 0 , 2 ], [ 3 , 5 ]), Circle ( 3 , [ 0 , 0 ], [ 0 , pi / 2 ]) subplot ( 121 ) L . draw () L . extend ( 0.1 ) . draw () axis ( 'square' ); subplot ( 122 ) C . draw () C . extend ( 0.1 ) . draw () axis ( 'square' ); # 扩展曲线 # Naive的扩展方式 设定好定向，对应图像轮廓由一系列矢量弧段$a_1, a_2,\\ldots a_n$组成 普通段普通扩展$a_i\\rightarrow a_i'$ 区分角度增加/减少的区别 角度 增加 的时候半径为正$r=|r|$ 角度 减少 的时候半径为负$r=-|r|$ 做变换$r\\rightarrow r'=r+d$ In [3]: c = triangloid ( 1 , 1 , 0.2 , - 0.4 ) c . draw ( label = 'colloid' ) title ( 'Naive idea' ) for seg in c : seg . extend ( 0.1 ) . draw () axis ( 'equal' ); # 处理转折点 如果顺拐，即$a_i\\times a_{i+1}>0$，转折段扩展出圆弧 如果逆拐，即$a_i\\times a_{i+1}<0$，转折段可以扩展出圆弧，但是直线更适合处理，因为终究需要剪除 In [4]: c . draw ( label = 'colloid' ) title ( 'Considering Turning points' ) c . offset_raw ( 0.1 ) . draw_each () axis ( 'equal' ); # 终极扩展方式 问题 ：我们现在的曲线，已经包含了扩展后的边界，但是有部分面积被环绕了多次 解决方法 ：剪除重复的正面积，保留真正的边界 简化 方法：即最大的正边界 BUG : 某些凹陷严重情形下扩展会有bug，蚀刻时也会有bug In [10]: off = linspace ( 0 , 0.4 , 5 ) for o in off : c . offset ( o ) . draw ( label = 'Offset {:.1f} ' . format ( o )) legend ( loc = \"upper right\" ) axis ( 'equal' ); # 曲线相交 Depletion把两个当成一个整体 In [11]: import pandas as pd area = [ c . offset ( o ) . area () for o in off ] peri = [ c . offset ( o ) . perimeter () for o in off ] pd . DataFrame ({ \"Offset\" : off , \"Area\" : area , \"Perimeter\" : peri }) Out[11]: Area Offset Perimeter 0 2.423661 0.0 7.423733 1 3.170664 0.1 7.516340 2 3.926929 0.2 7.608948 3 4.699730 0.3 7.938916 4 5.521669 0.4 8.510362","tags":"Physics","url":"https://www.peijun.me/two-dimension-depletion-force-between-colloids.html"},{"title":"拼图游戏之打乱拼图","text":"原理请参见 拼图游戏之可解性条件 # 线性时间计算排列的奇偶性 # 计算交换次数（鸠占鹊巢） 扫描所有的鸟，命名为鸠(Dove): 鸠现在占着鹊(Magpie)巢 鸠的巢(nest)被麻雀(sparrow)占领 鸠和麻雀换巢：鸠回到自己的巢，麻雀挪到鹊的巢 In [1]: def rev_index ( l ): '''Construct nest<->bird reversed index''' rev = empty_like ( l ) for i , j in enumerate ( l ): rev [ j ] = i return rev def count_swap ( nest2bird , bird2nest ): '''Count swaps needed by an order''' count = 0 for dove , magpie in enumerate ( bird2nest ): if dove != magpie : count += 1 sparrow = nest2bird [ dove ] nest2bird [ magpie ] = sparrow bird2nest [ sparrow ] = magpie return count # 根据奇偶性判断拼图的可解性 In [2]: def isSolvable ( puzzle ): '''Judge solvability of 2D matrix A, with largest elem representing empty O''' m , n = puzzle . shape empty = puzzle . size - 1 nest2bird = puzzle . flatten () bird2nest = rev_index ( nest2bird ) ds = sum ( divmod ( empty - bird2nest [ empty ], n )) cnt = count_swap ( nest2bird , bird2nest ) return ( cnt + ds ) % 2 == 0 # 生成拼图 先随机洗牌，然后如果奇偶性错误则对换一组不含空格的块 In [3]: def puzzle ( m , n = None , shufall = True ): if not n : n = m if ( n < 2 or m < 2 ): raise Exception ( 'Puzzle too small' ) A = arange ( m * n ) if shufall : shuffle ( A ) else : shuffle ( A [: - 1 ]) A = A . reshape ([ m , n ]) if not isSolvable ( A ): if A [ 0 , 0 ] != A . size - 1 and A [ 0 , 1 ] != A . size - 1 : A [ 0 , 0 ], A [ 0 , 1 ] = A [ 0 , 1 ], A [ 0 , 0 ] else : A [ 1 , 0 ], A [ 1 , 1 ] = A [ 1 , 1 ], A [ 1 , 0 ] return A # 生成的例子 In [4]: puzzle ( 6 , shufall = False ) # 最后一个格子不变 Out[4]: array([[30, 6, 0, 4, 21, 9], [ 3, 25, 18, 17, 24, 34], [12, 10, 29, 5, 32, 22], [27, 13, 23, 26, 16, 15], [19, 2, 1, 11, 33, 8], [28, 7, 20, 14, 31, 35]]) In [5]: puzzle ( 6 ) # 最后一个格子位置随机 Out[5]: array([[ 9, 15, 25, 34, 13, 22], [28, 26, 19, 8, 12, 35], [16, 21, 33, 0, 27, 17], [ 1, 24, 23, 6, 29, 5], [10, 2, 4, 7, 14, 20], [30, 32, 18, 31, 11, 3]])","tags":"Puzzles","url":"https://www.peijun.me/shuffle-npuzzles.html"},{"title":"拼图游戏之可解性条件","text":"# 可解的必要条件 游戏中只能由不断的空块和相邻的块做交换完成。而可解性也由交换数的奇偶性决定，交换数定义为通过交换任意块使得其恢复原状需要的次数。 考虑 包含空块 在内的交换数：空格子移动一步的时候，进行了一次交换，交换数的奇偶性发生改变。显然最后的奇偶性取决于空格移动步数的奇偶性。而移动步数奇偶性不依赖于具体路径，只依赖于初末位置。我们可以找一条最简单的路径的长度（曼哈顿距离）的奇偶性来判断可解性。 必要条件 移动步数奇偶性必须等于排列数/交换数的奇偶性 # 充分性 # 三元轮换与聚合 我们考虑三元置换的可行性。首先如果我们有$ABC$和空格聚在一起，那么显然很容易将$ABC$做轮换$R$ A B C A B C --> --> C O B O A O 对于大于$(2,2)$显然是容易聚在一起的，而且方块越多，自由度越大，越容易聚在一起。所以这里我们仅考虑$(3,2)$的情况: xx Bx Bx AB AB AB xx -> xx -> Ax -> xx -> Cx -> CO xx xx xx xx xx xx 对于任意的三个$ABC$，我们设想： 聚合Fusion：先将$ABC$和空格$O$，经过系列操作$F=F_1F_2\\cdots F_n$拼到一起 三元轮换Rotation：$R$ 拆散Decomposition：$F'=F&#94;{-1}$ 至此，我们完成了三元轮换。 # 双对换 三元轮换$ABC\\rightarrow BCA$相当于$AB$和$BC$两对带交叉的对换 通过两个三元轮换$ABC$以及$CBD$，那么可以构造无交叉对换： $$ABCD\\rightarrow CABD\\rightarrow BADC$$ 结果是$AB$对换，$CD$对换。另外无交叉对换显然可以构造轮换：$$ABCDE\\rightarrow BACED\\rightarrow BCADE$$ 显然轮换和双对换在四块以上时是等价的。 # 结论 通过多次双对换/轮换，显然只要是偶逆序数(亦即偶交换数)都可以恢复初始值。 # 复杂度 大小为$n&#94;2$的拼图，每次对换平均大概需要$3n$复杂度，总共有$n&#94;2$块，对应大致需要$n&#94;2$次对换。总的时间复杂度为$n&#94;3$ # 例子 对于$12$和$34$同时错位，首先我们轮换$214$: 2 1 2 1 4 2 4 2 4 3 F 4 O R 1 O F' 1 3 5 O 5 3 5 3 5 O 然后我们轮换$143$，合并过程$F=F_1+F_2$ 4 2 1 4 1 4 3 1 1 2 1 3 F1 5 2 F2 3 O R 4 O F' 3 4 5 O O 3 2 5 2 5 5 O 合并两个过程，省略第一个$F'$，我们有： 2 1 2 1 4 2 1 4 3 1 1 2 12 4 3 F1 4 O R1 1 O F2 3 O R2 4 O F2' 3 O F1' 34 5 O 5 3 5 3 2 5 2 5 5 4 5O 亦即$$\\mathrm{Swap}(1,2)\\mathrm{Swap}(3,4)=F_1R_1F_2R_2F_2'F_1'$$ # 生成更多的拼图? 参见 拼图游戏之打乱拼图","tags":"Puzzles","url":"https://www.peijun.me/npuzzle-solvability.html"},{"title":"配置Git","text":"# 配置文件 git config --global user.name \"Peijun Zhu\" git config --global user.email \"zpj.ustc@gmail.com\" git config --global tar.tar.xz.command \"xz -c\" git config --global core.excludesfile ~/.gitignore_global # 生成/添加ssh-key ssh-keygen -t rsa -b 4096 -C \"zpj.ustc@gmail.com\" cat id_rsa.pub 再把生成的内容贴入Github/Bitbucket等托管网站","tags":"Computer","url":"https://www.peijun.me/git-configuration.html"},{"title":"背单词软件Parley介绍","text":"Parley是一个帮助你记忆的程序，对我们GRE党来说，最重要的莫过于记忆单词了。 # 安装 在终端中输入 sudo zypper in parley 即可安装好Parley。 但是Parley是一个程序而已，我们还需要下载相关的词库作为数据支持。 # 获取词库 Parley本身自带的词库非常少，但是他的词库可扩展性非常强。 # 从xls等表格转化其他词汇表 尽管没有多少人用过Parley，然而有不少前辈有电子版的红宝书、要你命3000等文档。 和Parley格式最接近的莫非xls这一类用 \\t 分割的表格文件了，于是你上网搜索\"GRE 红宝书 xls\"，\"再要你命 3000 xls\"等就能找到相关的词汇表文件了。之后然后复制粘贴到Parley中再保存你就有词库了！ # 处理转换其他格式文件 你想要背的词汇表网上找不到xls，但是能找到具有某种格式的txt/doc。这时候就要祭出终极大杀器——正则表达式了。进行一系列替换后，你最终得到了你想要的词库。或者你像我一样并不精通正则表达式可以写一个Python脚本来实现格式转换、解析为Parley的 xml 格式 # 用Goldendict配合Parley 设置好Goldendict屏幕取词后就可以对 背单词界面 的单词进行取词了 # 优缺点 可以自由设置各种选项，灵活性好 Parley可以显示音标，而音标可以通过程序自动添加（尽管音标库并不完善） 用Goldendict取词功能可以方便的对单词进行查询，获取读音释义等信息 没法自动读出来读音，尽管可以用Goldendict稍微弥补 背单词的算法是：复习时熟词优先级高，熟词出现的时间间隔较大，这样使得熟词像滚雪球一样越来越大！ 编辑功能太弱，不过你自己可以写Kross脚本编辑，虽然这种插件语言我并不知道有哪些API 源文档是纯文本文件，即使不会使用Kross脚本也可以写Python等脚本进行相应处理 # 背单词策略——追求快速反应 使用Parley背单词，背了几个假期。最后Verbal考了158（然并暖）。个人的一些经验： 如果需要想一会才能知道意思请果断点击不知道，并看它的释义是否熟悉 这还有一个好处是，对生词可以快速建立影响而不是浪费时间瞎猜 复习的快可以极大的加快你的刷词速度 瞎猜而总是猜错会导致一种沮丧感，从而影响你背单词的情绪","tags":"Computer","url":"https://www.peijun.me/intro-to-parley.html"},{"title":"Energy of Electrostatic Field in a Triangle","text":"Energy of a triangle \\(\\newcommand{\\bm}{\\mathbf} \\newcommand{\\pp}{\\partial}\\) For a triangle \\(ABC\\) , assume the points have potential \\(\\varphi_a,\\varphi_b,\\varphi_c\\) . As shown in Fig. ???, \\(AB'\\perp AC, AC'\\perp AB\\) , combine the components of \\(\\nabla\\varphi\\) , we have $$(\\nabla\\varphi)&#94;2=\\frac{1}{\\sin&#94;2 A}\\left(\\frac{\\varphi_{ab}&#94;2}{c&#94;2}+\\frac{\\varphi_{ac}&#94;2}{b&#94;2}-\\frac{2\\varphi_{ab}\\varphi_{ac}\\cos A}{bc}\\right)$$ The area of triangle is $$S=\\frac{bc\\sin A}{2}$$ The energy of this triangle is $$\\begin{aligned} E_{\\triangle}&\\propto S(\\nabla\\varphi)&#94;2\\\\ &\\propto \\frac{bc}{\\sin A}\\left(\\frac{\\varphi_{ab}&#94;2}{c&#94;2}+\\frac{\\varphi_{ac}&#94;2}{b&#94;2}-\\frac{2\\varphi_{ab}\\varphi_{ac}\\cos A}{bc}\\right)\\end{aligned}$$ 对 \\(\\varphi_a\\) 求导得到： $$\\frac{\\pp E_\\triangle}{\\pp \\varphi_a}\\propto \\left(\\frac{b}{c\\sin A}-\\cot A\\right)\\varphi_{ab}+\\left(\\frac{c}{b\\sin A}-\\cot A\\right)\\varphi_{ac}\\label{a}$$ 其中： $$\\frac{b}{c\\sin A}-\\cot A=\\frac{b-c\\cos A}{c\\sin A}=\\frac{a\\cos C}{a\\sin C}=\\cot C$$ 同理 $$\\frac{c}{b\\sin A}-\\cot A=\\cot B$$ 故偏导化为 $$\\frac{\\pp E_\\triangle}{\\pp \\varphi_a}\\propto \\varphi_{ab}\\cot C+\\varphi_{ac}\\cot B \\label{comp}$$ Stationary point equation for energy minimum Let \\(E\\) be the total energy and use \\(i\\) to mark all triangles containing \\(A\\) . \\(B_i, C_i\\) are other points in triangle \\(i\\) . In the stationary point, we have $$\\frac{\\pp E}{\\pp \\varphi_a}=\\sum_i\\frac{\\pp E_i}{\\pp \\varphi_a} \\propto\\sum_i \\varphi_{ab_i}\\cot C_i+\\varphi_{ac_i}\\cot B_i=0$$ So, $$\\sum_i\\left(\\cot C_i+\\cot B_i\\right)\\varphi_{a}=\\sum_i\\left( \\varphi_{b_i}\\cot C_i+\\varphi_{c_i}\\cot B_i\\right)$$ $$\\Rightarrow\\varphi_a=\\frac{\\sum_i \\varphi_{b_i}\\cot C_i+\\varphi_{c_i}\\cot B_i}{\\sum_i\\cot C_i+\\cot B_i}$$ Assume we know coordinates of a triangle \\(ABC\\) . To calculate \\(\\cot A\\) , we define \\(\\bm b=\\overrightarrow{AB}, \\bm c=\\overrightarrow{AC}\\) . $$\\cot A=\\frac{bc\\cos A}{bc\\sin A}=\\frac{\\bm{b\\cdot c}}{|\\bm{b\\times c}|}=\\frac{\\bm{b\\cdot c}}{2S}$$ The denominator \\(2S\\) is the same for a triangle.","tags":"Physics","url":"https://www.peijun.me/energy-of-electrostatic-field-in-a-triangle.html"},{"title":"Integrable Constraint","text":"\\(\\newcommand{\\bm}{\\mathbf}\\newcommand{\\dd}{\\mathrm{d}}\\newcommand{\\pp}{\\mathrm{p}}\\) In N dimension, $$\\begin{aligned} \\bm f\\times\\bm g&\\rightarrow f_a\\times g_b=[f_ag_b]_{ab}\\\\ \\bm f\\cdot\\bm g\\times\\bm h&\\rightarrow(f_a, g_b, h_c)=[f_ag_bh_c]_{abc} \\end{aligned}$$ Suppose \\(\\bm f(\\bm r)=f_i\\hat e_i\\) , and a constraint \\(f_i\\dd r_i=0\\) Even if the \\(\\nabla\\times\\bm f\\neq 0\\) , we may find some function \\(\\varphi\\) which is nonzero at the nonzero points of \\(\\bm f\\) s.t. $$\\nabla\\times(\\varphi\\bm f)=\\nabla\\varphi\\times\\bm f+\\varphi\\nabla\\times\\bm f=0$$ Dotted by \\(\\bm f\\) , we find $$\\varphi\\bm f\\cdot\\nabla \\times\\bm f=(\\bm f,\\nabla\\varphi, \\bm f)\\propto [f_i\\pp_j f_k]_{ijk} =0$$ E.g. For a 2D function, \\(A_{ijk}\\equiv 0\\) , so there is always a solution.","tags":"Physics","url":"https://www.peijun.me/integrable-constraint.html"},{"title":"班级讲座——LaTeX排版介绍","text":"这是在班级LaTeX讲座上的 LaTeX简介 幻灯片，内容涵盖: LaTeX基本历史 LaTeX基本结构 排版学的基础知识 数学公式 图表 宏 相关的USTC个人主页链接已经失效。幻灯片源代码已亡佚。","tags":"Computer","url":"https://www.peijun.me/LaTeX-intro-lecture.html"},{"title":"NaCl离子晶体晶格能的研究","text":"这是我的大一化学原理小论文 离子晶体晶格能，主要成分为静电势能，计算出每个离子的电势能后就可以估算出其晶格能。由于晶体结构是无限延伸的,所以必然要求一些无穷级数的和。这个级数难以求出精确值。由实际意义知其必然收敛,因此可以用计算机进行相应计算。对于NaCl晶体,设相邻异性离子产生的电势能为$E_0$，$E_1$是某个离子和其他离子作用的电势能。定义Madelung常数$k=E_1/E_0$。编写C语言程序计算出$$k=1.74756459463, \\quad E_1=kE_0=-8.95\\rm{eV}$$ 设晶体有$2N$个离子，则总能量为$NE_1$ #include <stdio.h> #include <math.h> #define N 1000 //计算的层数 int main (){ double a = 0. , b = 0. , c = 0. , s = 0. , t = 0. ; int m , i , j ; for ( m = 1 ; m < N + 1 ; m ++ ){ a = ( 1 - ( m % 2 ) * 2 ) / ( double ) m ; //*6 面 b = ( 1 - ( m % 2 ) * 2 ) / sqrt ( 3 * m * m ); //*8 点 c = 1 / sqrt ( 2 * m * m ); //*12 边 for ( i = 1 ; i < m ; i ++ ){ for ( j = 0 ; j < m ; j ++ ){ a += 4 * ( 1 - (( m + i + j ) % 2 ) * 2 ) / sqrt ( m * m + i * i + j * j ); //*24 } c += 2 * ( 1 - ( i % 2 ) * 2 ) / sqrt ( 2 * m * m + i * i ); } t = a * 6 + b * 8 + c * 12 ; if ( m % 10 == 0 ){ printf ( \"t[%4d]=%9lf \\t\\t s=%.15lf \\t\\t s2=%lf \\n \" , m , t , s + a * 3 + b + c * 3 , s + t ); } s += t ; } s = s - t + a * 3 + b + c * 3 ; printf ( \"s[%4d]=%.15lf \\n \" , N , s ); return 0 ; } # Cython重写 date: 2017-05-05 In [2]: %% cython cimport cython from libc.math cimport sqrt @cython . cdivision ( True ) cpdef double madelung ( int N ): cdef double face , vertex , edge , E = 0. cdef int m , i , j for m in range ( 1 , N + 1 ): vertex = ( 1 - ( m % 2 ) * 2 ) / sqrt ( 3 * m * m ) edge = 1 / sqrt ( 2 * m * m ) face = ( 1 - ( m % 2 ) * 2 ) / < double > m for i in range ( 1 , m ): for j in range ( m ): face += 4 * ( 1 - (( m + i + j ) % 2 ) * 2 ) / \\ sqrt ( m * m + i * i + j * j ) edge += 2 * ( 1 - ( i % 2 ) * 2 ) / sqrt ( 2 * m * m + i * i ) E += face * 6 + vertex * 8 + edge * 12 E -= face * 3 + 7 * vertex + 9 * edge return E In [3]: E = array ([ madelung ( 2 ** i ) for i in range ( 10 )]) y = log2 ( abs ( diff ( E ))) In [4]: plot ( y , 'o-' ) xlabel ( r \"$\\log_2 N$\" ) ylabel ( r \"$\\log_2 \\delta E$\" ); In [5]: E [ - 1 ] Out[5]: -1.7475645946340816","tags":"Physics","url":"https://www.peijun.me/nacl-madelung-constant.html"}]}